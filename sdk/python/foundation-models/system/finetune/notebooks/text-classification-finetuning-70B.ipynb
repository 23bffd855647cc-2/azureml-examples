{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44210d88",
   "metadata": {
    "gather": {
     "logged": 1690422784786
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.0.1\n",
    "# !pip install bitsandbytes\n",
    "# !pip install transformers==4.31.0\n",
    "# !pip install peft\n",
    "# !pip install azureml-evaluate-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1833f26f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd457e1",
   "metadata": {
    "gather": {
     "logged": 1690422786827
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer version: 4.31.0, torch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "print(f'transformer version: {transformers.__version__}, torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cae5aa0",
   "metadata": {
    "gather": {
     "logged": 1690422788582
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('azureml'), PosixPath('//eastus.api.azureml.ms/mlflow/v1.0/subscriptions/9ec1d932-0f3f-486c-acc6-e7d78b358f9b/resourceGroups/aml_shared/providers/Microsoft.MachineLearningServices/workspaces/aml_shared_eus_ws')}\n",
      "  warn(msg)\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//127.0.0.1'), PosixPath('46808/MSI/auth')}\n",
      "  warn(msg)\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//127.0.0.1'), PosixPath('46808/OBO/token')}\n",
      "  warn(msg)\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Users/arjsingh/Finetuning_70B/text-classification-finetuning-70B-int8-with-peft-lora-arjs.ipynb')}\n",
      "  warn(msg)\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import LlamaTokenizerFast, LlamaForCausalLM, LlamaTokenizer, LlamaForSequenceClassification\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from azureml.metrics import compute_metrics, constants\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f5c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load original tokenizer\n",
    "tokenizer_path = '../70b_hf_cnverted'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf2f500",
   "metadata": {
    "gather": {
     "logged": 1690424906644
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [33:53<00:00, 135.56s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ../70b_hf_cnverted and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load original model\n",
    "model_path = '../70b_hf_cnverted'\n",
    "model = LlamaForSequenceClassification.from_pretrained(model_path, device_map='auto', load_in_8bit=True, torch_dtype=torch.float16, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1f5e5",
   "metadata": {},
   "source": [
    "### Dataset 20-News group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34469c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"text-dnn-data\"  # Local directory to store data\n",
    "# blobstore_datadir = data_dir  # Blob store directory to store data in\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "target_column_name = \"label\"\n",
    "feature_column_name = \"sentence\"\n",
    "\n",
    "\n",
    "def get_20newsgroups_data():\n",
    "    \"\"\"Fetches 20 Newsgroups data from scikit-learn\n",
    "    Returns them in form of pandas dataframes\n",
    "    \"\"\"\n",
    "    remove = (\"headers\", \"footers\", \"quotes\")\n",
    "    categories = [\n",
    "        \"rec.sport.baseball\",\n",
    "        \"rec.sport.hockey\",\n",
    "        \"comp.graphics\",\n",
    "        \"sci.space\",\n",
    "    ]\n",
    "\n",
    "    data = fetch_20newsgroups(\n",
    "        subset=\"train\",\n",
    "        categories=categories,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        remove=remove,\n",
    "    )\n",
    "    data = pd.DataFrame(\n",
    "        {feature_column_name: data.data, target_column_name: data.target}\n",
    "    )\n",
    "\n",
    "    data_train = data[:200]\n",
    "    data_test = data[200:300]\n",
    "\n",
    "    data_train = remove_blanks_20news(\n",
    "        data_train, feature_column_name, target_column_name\n",
    "    )\n",
    "    data_test = remove_blanks_20news(data_test, feature_column_name, target_column_name)\n",
    "    return Dataset.from_pandas(data_train), Dataset.from_pandas(data_test)\n",
    "\n",
    "\n",
    "def remove_blanks_20news(data, feature_column_name, target_column_name):\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        data.at[index, feature_column_name] = (\n",
    "            row[feature_column_name].replace(\"\\n\", \" \").strip()\n",
    "        )\n",
    "\n",
    "    data = data[data[feature_column_name] != \"\"]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9701254",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = get_20newsgroups_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb90295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', '__index_level_0__'],\n",
       "    num_rows: 193\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c030a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', '__index_level_0__'],\n",
       "    num_rows: 98\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2678209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a877b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 193/193 [00:00<00:00, 865.48 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 858.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_train.map(\n",
    "    lambda samples: tokenize(samples), remove_columns=[\"__index_level_0__\", \"sentence\"], load_from_cache_file=False)\n",
    "\n",
    "validation_dataset = data_test.map(\n",
    "    lambda samples: tokenize(samples), remove_columns=[\"__index_level_0__\", \"sentence\"], load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2b5d142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 98)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574a9744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'input_ids': [1, 360, 29943, 29956, 471, 8688, 411, 278, 6850, 29903, 297, 3458, 313, 4716, 2289, 2099, 1407, 2217, 467, 29871, 18927, 310, 1009, 4688, 12089, 5518, 750, 20407, 411, 263, 528, 4774, 280, 25325, 322, 1023, 470, 2211, 4045, 20043, 701, 304, 29341, 29889, 29871, 306, 4140, 896, 892, 1811, 304, 22884, 920, 12862, 278, 4799, 637, 471, 29889, 29871, 1152, 27043, 4072, 29901, 29871, 1954, 22094, 278, 6216, 4989, 412, 29894, 457, 322, 6600, 1747, 723, 367, 2534, 565, 278, 528, 4774, 280, 399, 3289, 25325, 472, 360, 29943, 29956, 29889, 313, 2831, 278, 1791, 29892, 896, 526, 5279, 2534, 777, 3081, 10205, 793, 1546, 278, 4799, 637, 322, 18830, 14368, 467, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(validation_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "885174ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=28620, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=8192, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test dataset\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52858cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  0\n",
      "Processing:  1\n",
      "Processing:  2\n",
      "Processing:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metrics skipped due to missing y_pred_proba:\n",
      " ['AUC_weighted', 'accuracy_table', 'average_precision_score_weighted', 'norm_macro_recall', 'average_precision_score_micro', 'average_precision_score_macro', 'average_precision_score_binary', 'log_loss', 'AUC_macro', 'AUC_binary', 'AUC_micro']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 3, 3]\n",
      "[3, 1, 1, 3]\n",
      "{'f1_score_micro': 0.25, 'f1_score_macro': 0.13333333333333333, 'precision_score_micro': 0.25, 'recall_score_binary': nan, 'weighted_accuracy': 0.3, 'accuracy': 0.25, 'precision_score_binary': nan, 'recall_score_micro': 0.25, 'recall_score_weighted': 0.25, 'f1_score_binary': nan, 'precision_score_weighted': 0.375, 'balanced_accuracy': 0.1111111111111111, 'precision_score_macro': 0.16666666666666666, 'matthews_correlation': -0.2886751345948129, 'f1_score_weighted': 0.30000000000000004, 'recall_score_macro': 0.1111111111111111}\n"
     ]
    }
   ],
   "source": [
    "# Metrics Computation\n",
    "device = \"cuda\"\n",
    "l = len(validation_dataset)\n",
    "batch_size = 1\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i in range(0, 4, batch_size):\n",
    "    print('Processing: ', i)\n",
    "    data_batch = validation_dataset[i:i + batch_size]\n",
    "    # NOTE: Before passing data_batch['input_ids] to the model, cast them using torch.LongTensor()\n",
    "    # Same for data_batch['attention_mask']. So that .to(device) call can work.\n",
    "    #print(data_batch)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=torch.LongTensor(data_batch['input_ids']).to(device), \n",
    "                        attention_mask=torch.LongTensor(data_batch['attention_mask']).to(device))\n",
    "    batch_predictions = outputs.logits.argmax(dim=-1)\n",
    "    batch_predictions, batch_references = batch_predictions.detach().cpu().numpy().tolist(), data_batch[\"label\"]\n",
    "    predictions.extend(batch_predictions)\n",
    "    references.extend(batch_references)\n",
    "\n",
    "print(predictions)\n",
    "print(references)\n",
    "\n",
    "#Compute metrics\n",
    "metrics = compute_metrics(task_type=constants.Tasks.CLASSIFICATION,\n",
    "                          y_test=predictions,\n",
    "                          y_pred=references)[\"metrics\"]\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fe0aa4",
   "metadata": {
    "gather": {
     "logged": 1690424906849
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 0,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 0,\n",
       " 'model.layers.2': 0,\n",
       " 'model.layers.3': 0,\n",
       " 'model.layers.4': 0,\n",
       " 'model.layers.5': 0,\n",
       " 'model.layers.6': 0,\n",
       " 'model.layers.7': 0,\n",
       " 'model.layers.8': 0,\n",
       " 'model.layers.9': 1,\n",
       " 'model.layers.10': 1,\n",
       " 'model.layers.11': 1,\n",
       " 'model.layers.12': 1,\n",
       " 'model.layers.13': 1,\n",
       " 'model.layers.14': 1,\n",
       " 'model.layers.15': 1,\n",
       " 'model.layers.16': 1,\n",
       " 'model.layers.17': 1,\n",
       " 'model.layers.18': 1,\n",
       " 'model.layers.19': 1,\n",
       " 'model.layers.20': 2,\n",
       " 'model.layers.21': 2,\n",
       " 'model.layers.22': 2,\n",
       " 'model.layers.23': 2,\n",
       " 'model.layers.24': 2,\n",
       " 'model.layers.25': 2,\n",
       " 'model.layers.26': 2,\n",
       " 'model.layers.27': 2,\n",
       " 'model.layers.28': 2,\n",
       " 'model.layers.29': 2,\n",
       " 'model.layers.30': 2,\n",
       " 'model.layers.31': 3,\n",
       " 'model.layers.32': 3,\n",
       " 'model.layers.33': 3,\n",
       " 'model.layers.34': 3,\n",
       " 'model.layers.35': 3,\n",
       " 'model.layers.36': 3,\n",
       " 'model.layers.37': 3,\n",
       " 'model.layers.38': 3,\n",
       " 'model.layers.39': 3,\n",
       " 'model.layers.40': 3,\n",
       " 'model.layers.41': 3,\n",
       " 'model.layers.42': 4,\n",
       " 'model.layers.43': 4,\n",
       " 'model.layers.44': 4,\n",
       " 'model.layers.45': 4,\n",
       " 'model.layers.46': 4,\n",
       " 'model.layers.47': 4,\n",
       " 'model.layers.48': 4,\n",
       " 'model.layers.49': 4,\n",
       " 'model.layers.50': 4,\n",
       " 'model.layers.51': 4,\n",
       " 'model.layers.52': 4,\n",
       " 'model.layers.53': 5,\n",
       " 'model.layers.54': 5,\n",
       " 'model.layers.55': 5,\n",
       " 'model.layers.56': 5,\n",
       " 'model.layers.57': 5,\n",
       " 'model.layers.58': 5,\n",
       " 'model.layers.59': 5,\n",
       " 'model.layers.60': 5,\n",
       " 'model.layers.61': 5,\n",
       " 'model.layers.62': 5,\n",
       " 'model.layers.63': 5,\n",
       " 'model.layers.64': 6,\n",
       " 'model.layers.65': 6,\n",
       " 'model.layers.66': 6,\n",
       " 'model.layers.67': 6,\n",
       " 'model.layers.68': 6,\n",
       " 'model.layers.69': 6,\n",
       " 'model.layers.70': 6,\n",
       " 'model.layers.71': 6,\n",
       " 'model.layers.72': 6,\n",
       " 'model.layers.73': 6,\n",
       " 'model.layers.74': 6,\n",
       " 'model.layers.75': 7,\n",
       " 'model.layers.76': 7,\n",
       " 'model.layers.77': 7,\n",
       " 'model.layers.78': 7,\n",
       " 'model.layers.79': 7,\n",
       " 'model.norm': 7,\n",
       " 'score': 7}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef0b2161",
   "metadata": {
    "gather": {
     "logged": 1690424907123
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=28620, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=8192, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d61b6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=28620, out_features=8192, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=8192, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e75f1740",
   "metadata": {
    "gather": {
     "logged": 1690424936083
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,192,000 || all params: 68,722,761,728 || trainable%: 0.011920359127043492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#peft_model = prepare_model_for_kbit_training(model)\n",
    "model = prepare_model_for_int8_training(model)\n",
    "peft_model = model\n",
    "\n",
    "config = LoraConfig(\n",
    "   r=4,\n",
    "   lora_alpha=16,\n",
    "   target_modules= [\n",
    "       \"q_proj\",\n",
    "       \"v_proj\",\n",
    "   ],\n",
    "   lora_dropout=.05,\n",
    "   bias=\"none\",\n",
    "   task_type=\"SEQ_CLS\", # use this to get the task type: https://github.com/huggingface/peft/blob/96c0277a1b9a381b10ab34dbf84917f9b3b992e6/src/peft/utils/config.py#L38\n",
    ")\n",
    "#config.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "peft_model = get_peft_model(peft_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1293757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForSequenceClassification(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-79): 80 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear8bitLt(\n",
      "                in_features=8192, out_features=8192, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
      "              (v_proj): Linear8bitLt(\n",
      "                in_features=8192, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
      "              (up_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
      "              (down_proj): Linear8bitLt(in_features=28620, out_features=8192, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (score): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=8192, out_features=4, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=8192, out_features=4, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fcfd609",
   "metadata": {
    "gather": {
     "logged": 1690424937542
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70ccc08c",
   "metadata": {
    "gather": {
     "logged": 1690424937704
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=0,\n",
    "    num_train_epochs=1,\n",
    "#     max_steps=16,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=False,\n",
    "    #bf16=True, #False, # Setting mixed precision to true also works without quantization\n",
    "    #optim=\"adamw_torch_fused\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    output_dir='.',\n",
    "    ddp_find_unused_parameters=None,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "                  model=peft_model,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=validation_dataset,\n",
    "                  args=training_args,\n",
    "                 )\n",
    "#peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39b837e5",
   "metadata": {
    "gather": {
     "logged": 1690419249413
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 09:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>1.400316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.006600</td>\n",
       "      <td>1.345799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.278500</td>\n",
       "      <td>1.329462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=1.044743628501892, metrics={'train_runtime': 607.9105, 'train_samples_per_second': 0.317, 'train_steps_per_second': 0.041, 'total_flos': 2.0295013204230144e+16, 'train_loss': 1.044743628501892, 'epoch': 1.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e66f7c6f-4afe-43c9-a13a-17bec5a1f94b",
   "metadata": {
    "gather": {
     "logged": 1690419447246
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 4) (98,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(validation_dataset)\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "874b3b15-b8b9-4ab3-aa91-dad01185b234",
   "metadata": {
    "gather": {
     "logged": 1690419447425
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "673546a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 2, 0, 3, 0, 3, 3, 2, 2, 0, 2, 2, 2, 0, 3, 1, 2, 2,\n",
       "       1, 3, 2, 3, 3, 2, 0, 3, 2, 1, 1, 1, 0, 3, 1, 0, 2, 2, 0, 2, 2, 2,\n",
       "       3, 1, 1, 3, 2, 0, 0, 0, 0, 2, 0, 1, 3, 1, 0, 0, 1, 3, 2, 2, 2, 2,\n",
       "       3, 2, 2, 3, 3, 2, 3, 1, 3, 0, 3, 0, 3, 1, 1, 3, 2, 3, 3, 0, 3, 1,\n",
       "       1, 1, 0, 2, 3, 2, 0, 3, 0, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ff3c3d2-72ba-4bd8-b7db-23f71ac44db7",
   "metadata": {
    "gather": {
     "logged": 1690419447869
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5306122448979592"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import evaluate\n",
    "# metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "# metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predictions.label_ids, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94421293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-79): 80 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=8192, out_features=8192, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=8192, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=8192, out_features=28620, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=28620, out_features=8192, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=8192, out_features=4, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=8192, out_features=4, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test dataset\n",
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07788b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  1\n",
      "Processing:  2\n",
      "Processing:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metrics skipped due to missing y_pred_proba:\n",
      " ['AUC_weighted', 'accuracy_table', 'average_precision_score_weighted', 'norm_macro_recall', 'average_precision_score_micro', 'average_precision_score_macro', 'average_precision_score_binary', 'log_loss', 'AUC_macro', 'AUC_binary', 'AUC_micro']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3]\n",
      "[3, 1, 1, 3]\n",
      "{'f1_score_micro': 0.5, 'f1_score_macro': 0.3333333333333333, 'precision_score_micro': 0.5, 'recall_score_binary': 0.5, 'weighted_accuracy': 0.5, 'accuracy': 0.5, 'precision_score_binary': 1.0, 'recall_score_micro': 0.5, 'recall_score_weighted': 0.5, 'f1_score_binary': 0.6666666666666666, 'precision_score_weighted': 1.0, 'balanced_accuracy': 0.25, 'precision_score_macro': 0.5, 'matthews_correlation': 0.0, 'f1_score_weighted': 0.6666666666666666, 'recall_score_macro': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# Metrics Computation\n",
    "device = \"cuda\"\n",
    "l = len(validation_dataset)\n",
    "batch_size = 1\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i in range(0, 4, batch_size):\n",
    "    print('Processing: ', i)\n",
    "    data_batch = validation_dataset[i:i + batch_size]\n",
    "    # NOTE: Before passing data_batch['input_ids] to the model, cast them using torch.LongTensor()\n",
    "    # Same for data_batch['attention_mask']. So that .to(device) call can work.\n",
    "    #print(data_batch)\n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model(input_ids=torch.LongTensor(data_batch['input_ids']).to(device), \n",
    "                             attention_mask=torch.LongTensor(data_batch['attention_mask']).to(device))\n",
    "    batch_predictions = outputs.logits.argmax(dim=-1)\n",
    "    batch_predictions, batch_references = batch_predictions.detach().cpu().numpy().tolist(), data_batch[\"label\"]\n",
    "    predictions.extend(batch_predictions)\n",
    "    references.extend(batch_references)\n",
    "\n",
    "print(predictions)\n",
    "print(references)\n",
    "\n",
    "#Compute metrics\n",
    "metrics = compute_metrics(task_type=constants.Tasks.CLASSIFICATION,\n",
    "                          y_test=predictions,\n",
    "                          y_pred=references)[\"metrics\"]\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075db610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acba78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
