{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Image Retrieval using Online Endpoints\n",
    "\n",
    "This sample shows how to deploy `embeddings` type models to an online endpoint for evaluating text to image retrieval embeddings.\n",
    " \n",
    "### Model\n",
    "Models that can perform the `embeddings` task are tagged with `embeddings`. We will use the `OpenAI-CLIP-Image-Text-Embeddings-vit-base-patch32` model in this notebook. If you opened this notebook from a specific model card, remember to replace the specific model name. If you don't find a model that suits your scenario or domain, you can discover and [import models from HuggingFace hub](../../import/import_model_into_registry.ipynb) and then use them for inference. \n",
    "\n",
    "### Outline\n",
    "1. Deploy model to online endpoint\n",
    "2. Parse the data in the expected format\n",
    "3. Extract embeddings using the endpoint\n",
    "4. Evaluate the embeddings for image retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deploy model to online endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    "    ClientSecretCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential)\n",
    "    subscription_id = workspace_ml_client.subscription_id\n",
    "    resource_group = workspace_ml_client.resource_group_name\n",
    "    workspace_name = workspace_ml_client.workspace_name\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace_name = \"<AML_WORKSPACE_NAME>\"\n",
    "workspace_ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace_name\n",
    ")\n",
    "\n",
    "# The models are available in the AzureML system registry, \"azureml\"\n",
    "registry_ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id,\n",
    "    resource_group,\n",
    "    registry_name=\"azureml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Pick a model to deploy\n",
    "\n",
    "Browse models in the Model Catalog in the AzureML Studio, filtering by the `embeddings` task. In this example, we use the `OpenAI-CLIP-Image-Text-Embeddings-vit-base-patch32` model. If you have opened this notebook for a different model, replace the model name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"OpenAI-CLIP-Image-Text-Embeddings-vit-base-patch32\"\n",
    "foundation_model = registry_ml_client.models.get(name=model_name, label=\"latest\")\n",
    "print(\n",
    "    f\"\\n\\nUsing model name: {foundation_model.name}, version: {foundation_model.version}, id: {foundation_model.id} for inferencing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Deploy the model to an online endpoint for real time inference\n",
    "\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    ")\n",
    "\n",
    "# Endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "timestamp = int(time.time())\n",
    "online_endpoint_name = \"clip-embeddings-\" + str(timestamp)\n",
    "# Create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + foundation_model.name\n",
    "    + \", for image-text-embeddings task\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import OnlineRequestSettings, ProbeSettings\n",
    "\n",
    "deployment_name = \"embeddings-mlflow-deploy\"\n",
    "\n",
    "# Create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=foundation_model.id,\n",
    "    instance_type=\"Standard_DS3_V2\",  # Use GPU instance type like Standard_NC6s_v3 for faster inference\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=1,\n",
    "        request_timeout_ms=90000,\n",
    "        max_queue_wait_ms=500,\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=49,\n",
    "        success_threshold=1,\n",
    "        timeout=299,\n",
    "        period=180,\n",
    "        initial_delay=180,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=10,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=10,\n",
    "        initial_delay=10,\n",
    "    ),\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parse Data in Expected Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Load data in expected data format\n",
    "\n",
    "The image+text dataset should have a corersponding json file with the following format:\n",
    "```json\n",
    "{\n",
    "    \"cloud_storage_directory_name\": \"<url to data storage>\",\n",
    "    \"annotated_images\": [\n",
    "        {\n",
    "            \"image_id\": \"<unique image id>\",\n",
    "            \"image_path_relative\": \"<image path relative to cloud_storage_directory_name>\",\n",
    "            \"texts\": [\n",
    "                {\"text_id\": \"<unique text id>\", \"text_content\": \"<sample text>\"},\n",
    "                {\"text_id\": \"<unique text id>\", \"text_content\": \"<sample text>\"},\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```json\n",
    "{\n",
    "    \"cloud_storage_directory_name\": \"https://customerblobstore.blob.core.windows.net/datasets/fashiondata\",\n",
    "    \"annotated_images\": [\n",
    "        {\n",
    "            \"image_id\": \"i0\",\n",
    "            \"image_path_relative\": \"images/1007129816.jpg\",\n",
    "            \"texts\": [\n",
    "                {\"text_id\": \"t0\", \"text_content\": \"a long black evening gown\"},\n",
    "                {\"text_id\": \"t1\", \"text_content\": \"a formal black strapless dress\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"image_id\": \"i1\",\n",
    "            \"image_path_relative\": \"images/1009434119.jpg\",\n",
    "            \"texts\": [\n",
    "                {\"text_id\": \"t2\", \"text_content\": \"a short-sleeve shirt with floral print\"},\n",
    "                {\"text_id\": \"t3\", \"text_content\": \"a t-shirt with a floral pattern\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATASET_FILE_NAME = \"<DATASET JSON FILE>\"\n",
    "with open(DATASET_FILE_NAME, \"rt\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract embeddings using the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "IMAGE_EMBEDDINGS_FILE_NAME = \"./embeddings_image.json\"\n",
    "TEXT_EMBEDDINGS_FILE_NAME = \"./embeddings_text.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Define helper methods for extracting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_REQUEST_FILE_NAME = \"request.json\"\n",
    "\n",
    "def make_request_images(image_urls):\n",
    "    request_json = {\n",
    "        \"input_data\": {\n",
    "            \"columns\": [\"image\", \"text\"],\n",
    "            \"data\": [\n",
    "                [image_url, \"\"] for image_url in image_urls\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(_REQUEST_FILE_NAME, \"wt\") as f:\n",
    "        json.dump(request_json, f)\n",
    "\n",
    "\n",
    "def make_request_texts(texts):\n",
    "    request_json = {\n",
    "        \"input_data\": {\n",
    "            \"columns\": [\"image\", \"text\"],\n",
    "            \"data\": [\n",
    "                [\"\", text] for text in texts\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(_REQUEST_FILE_NAME, \"wt\") as f:\n",
    "        json.dump(request_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_and_save_embeddings(embeddings_file_name, images=True):\n",
    "    embedding_records = []\n",
    "\n",
    "    image_records = dataset[\"annotated_images\"]\n",
    "    for i in tqdm(range(0, len(image_records), BATCH_SIZE)):\n",
    "        # Get the current batch of image records.\n",
    "        j = min(len(image_records), i + BATCH_SIZE)\n",
    "        batch_image_records = image_records[i:j]\n",
    "\n",
    "        if images:\n",
    "            # Make the list of image urls and their ids in the current batch.\n",
    "            batch_image_urls = [\n",
    "                dataset[\"cloud_storage_directory_name\"] + \"/\" + r[\"image_path_relative\"].replace(\"\\\\\", \"/\")\n",
    "                for r in batch_image_records\n",
    "            ]\n",
    "            make_request_images(batch_image_urls)\n",
    "            batch_ids = [r[\"image_id\"] for r in batch_image_records]\n",
    "        else:\n",
    "            # Make the list of texts and their ids in the current batch.\n",
    "            # [FIXME] This may be larger than the batch size specified\n",
    "            batch_texts = [\n",
    "                t[\"text_content\"]\n",
    "                for r in batch_image_records if \"texts\" in r for t in r[\"texts\"]\n",
    "            ]\n",
    "            make_request_texts(batch_texts)\n",
    "            batch_ids = [t[\"text_id\"] for r in batch_image_records if \"texts\" in r for t in r[\"texts\"]]\n",
    "\n",
    "        # Call the endpoint and get the embeddings for the current batch.\n",
    "        response = workspace_ml_client.online_endpoints.invoke(\n",
    "            endpoint_name=online_endpoint_name,\n",
    "            deployment_name=deployment_name,\n",
    "            request_file=_REQUEST_FILE_NAME,\n",
    "        )\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except:\n",
    "            print(f\"did not get embeddings for batch {i}-{j}\")\n",
    "            print(response)\n",
    "            continue\n",
    "        output_field_name = \"image_features\" if images else \"text_features\"\n",
    "        batch_embeddings = [r[output_field_name] for r in response]\n",
    "\n",
    "        # Store the embeddings and the corresponding ids for the current batch.\n",
    "        id_field_name = \"image_id\" if images else \"text_id\"\n",
    "        for id_, e in zip(batch_ids, batch_embeddings):\n",
    "            embedding_records.append({id_field_name: id_, \"embedding\": e})\n",
    "\n",
    "    # Save embeddings to file.\n",
    "    with open(embeddings_file_name, \"wt\") as f:\n",
    "        json.dump(embedding_records, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Call methods for invoking endpoint and extracting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute image embeddings\n",
    "compute_and_save_embeddings(IMAGE_EMBEDDINGS_FILE_NAME, True)\n",
    "# compute text embeddings\n",
    "compute_and_save_embeddings(TEXT_EMBEDDINGS_FILE_NAME, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate Embeddings for image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Build index with image embeddings and query with text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "\n",
    "# read query embeddings file into sklearn index with brute force algorithm\n",
    "with open(IMAGE_EMBEDDINGS_FILE_NAME, \"r\") as f:\n",
    "    index_file_data = json.load(f)\n",
    "index_embeddings = [sample['embedding'] for sample in index_file_data]\n",
    "index = NearestNeighbors(algorithm='brute', metric='cosine')\n",
    "index.fit(index_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read query embeddings file into list of vectors\n",
    "with open(TEXT_EMBEDDINGS_FILE_NAME, \"r\") as f:\n",
    "    query_file_data = json.load(f)\n",
    "query_embeddings = [sample['embedding'] for sample in query_file_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Query the index and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance, result_ids = index.kneighbors(query_embeddings, n_neighbors=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_text_to_image_embeddings(dataset, result_ids, index_file_data, query_file_data):\n",
    "    # build text to image table\n",
    "    dataset_file_data = dataset[\"annotated_images\"]\n",
    "    text_to_image = {}\n",
    "    for record in dataset_file_data:\n",
    "        image_id = record[\"image_id\"]\n",
    "        for text in record[\"texts\"]:\n",
    "            text_to_image[text[\"text_id\"]] = image_id\n",
    "\n",
    "    # evaluate recall\n",
    "    recall_array = np.zeros(len(result_ids))\n",
    "    for idx, result in enumerate(result_ids):\n",
    "        index_records = [index_file_data[i] for i in result]\n",
    "        for index_record in index_records:\n",
    "            if text_to_image[query_file_data[idx][\"text_id\"]] == index_record[\"image_id\"]:\n",
    "                recall_array[idx] = 1\n",
    "                break\n",
    "        \n",
    "    return recall_array.sum() / recall_array.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = evaluate_text_to_image_embeddings(dataset, result_ids, index_file_data, query_file_data)\n",
    "print(\"R@{}={:.1f}\".format(K, 100.0 * recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc_133",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
