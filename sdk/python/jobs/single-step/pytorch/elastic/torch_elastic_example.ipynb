{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore distributed elastic training using PyTorch, with a focus on BERT pretraining using a large model size. We will also showcase the cost benefits of using low-cost, low-priority VMs on Azure Machine Learning.\n",
    "\n",
    "This notebook is intended for data scientists and machine learning practitioners who are familiar with PyTorch, and want to learn how to use Azure Machine Learning to run distributed elastic training jobs on Azure.\n",
    "\n",
    "\n",
    "## Requirements/Prerequisites\n",
    "- An Azure acoount with active subscription [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- Azure Machine Learning workspace [Configure workspace](../../../configuration.ipynb) \n",
    "- Python Environment\n",
    "- Install Azure ML Python SDK Version 2\n",
    "## Learning Objectives\n",
    "- Connect to workspace using Python SDK v2\n",
    "- Distributed training of Pytorch model using Torch Elastic\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import AmlCompute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Connect to workspace using DefaultAzureCredential\n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create a low-priority compute cluster\n",
    "\n",
    "In this section, we will create and configure a low-priority compute cluster on Azure Machine Learning. \n",
    "\n",
    "Training on [low-priority compute](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-optimize-cost#low-pri-vm) can cost much less than on dedicated compute.\n",
    "\n",
    "When creating a compute cluster, its priority must be specified. The priority can either be _dedicated_ (default) or _low priority_. (See [this API doc](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute.amlcomputeprovisioningconfiguration?view=azure-ml-py) for details.)\n",
    "\n",
    "A job using dedicated compute is granted uninterrupted access to a VM. In contrast, low-priority compute is temporary and may be preempted by higher priority jobs. When a low-priority job is preempted, it must stop running and wait for compute to become available again. When compute becomes available again, the job restarts from scratch on the new compute. To avoid starting from scratch every time a job is preempted and restarted, special handling is needed in the training script to save state between preemptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x23b1a8744f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_name = \"low-pri-cluster\"\n",
    "compute_cluster = AmlCompute(\n",
    "   name=compute_name, \n",
    "   description=\"Low priority compute cluster\",\n",
    "   size=\"Standard_NC6s_v3\",\n",
    "   min_instances=0, \n",
    "   max_instances=5,\n",
    "   tier='LowPriority'\n",
    ")\n",
    " \n",
    "ml_client.begin_create_or_update(compute_cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PyTorch Elastic Concepts\n",
    "\n",
    "In this section, we will explain the concepts of PyTorch Store and Rendezvous Backend."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Elastic is a framework for distributed training in PyTorch that allows you to scale your training jobs to multiple nodes and handle failures gracefully. Here are some of the key concepts in PyTorch Elastic:\n",
    "\n",
    "- **Rendezvous**: A rendezvous is a mechanism for nodes to discover each other and agree on a common set of parameters for the training job. In PyTorch Elastic, a rendezvous is typically implemented as a backend service that nodes can connect to and exchange information with. The rendezvous service is responsible for coordinating the nodes and ensuring that they all have the same view of the training job (e.g. the same set of parameters, node rank, etc.)\n",
    "\n",
    "- **RendezvousHandler**: A rendezvous handler is an object that encapsulates the logic for connecting to a rendezvous service and exchanging information with other nodes. In PyTorch Elastic, a rendezvous handler is responsible for creating a rendezvous and registering the current node with it. There are several different types of rendezvous handlers available in PyTorch Elastic, such as `EtcdStoreRendezvousHandler`, `TCPStoreRendezvousHandler`,  `FileStoreRendezvousHandler`.\n",
    "\n",
    "- **RendezvousBackend**: A rendezvous backend is the implementation of the rendezvous service. PyTorch Elastic provides several different rendezvous backends, such as `EtcdRendezvousBackend` or `FileStoreRendezvousBackend`.\n",
    "\n",
    "- **PyTorch Store**: A PyTorch store is a key-value store that is used to share data between nodes in a distributed training job. PyTorch Elastic provides a `TCPStore` implementation that uses a TCP socket to exchange data between nodes. PyTorch Elastic also provides a `FileStore` implementation that uses a file on disk to exchange data between nodes.\n",
    "\n",
    "These concepts work together to enable distributed training in PyTorch Elastic. A rendezvous backend is responsible for coordinating nodes and ensuring that they all have the same view of the training job. A rendezvous handler is responsible for creating a rendezvous and registering the current node with it. A PyTorch store is used to share data between nodes in the training job."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use a custom PyTorch Store and Rendezvous Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Is stateless the right word here?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training on dynamic or low priority clusters, using existing stores like TCPStore or EtcdStore can have some limitations. For example:\n",
    "\n",
    "- **Node failures**: If nodes can go down during training, the existing stores may not be able to handle the failure gracefully. This can result in lost data or inconsistent state across the nodes.\n",
    "\n",
    "- **Master node**: If the master node is not known upfront, it may be difficult to configure the existing stores to work with the cluster. For example, the master node may need to be manually specified in the configuration, which can be cumbersome if the cluster is dynamic.\n",
    "\n",
    "- **Additional infrastructure setup**: Using existing stores may require additional infrastructure setup, such as setting up a separate Etcd cluster or TCP server. This can add complexity to the training setup and increase the chances of failure.\n",
    "\n",
    "Creating a custom rendezvous backend or store that is stateless can simplify things in some cases. A stateless backend or store can be more resilient to node failures, since it does not rely on maintaining state across nodes. It can also be easier to configure, since it does not require setting up additional infrastructure.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Custom PyTorch Store and Rendezvous Backend \n",
    "\n",
    "In this notebook, we are going to use Azure Table Storage for storing rendezvous information and sharing data between nodes. Azure Table Storage is a NoSQL key-value store that is part of the Azure Storage service. It is a good fit for this scenario because it is a fully managed service that can scale to handle large amounts of data. It also supports atomic operations, which makes it easy to implement a rendezvous backend or store that is stateless.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Custom PyTorch Store\n",
    "\n",
    "The custom PyTorch store is implemented in the ``src/azure_table_store.py`` file.\n",
    "\n",
    "In this example, the `AzureTableStore` class implements the `Store` interface and uses the Azure Data Table API to store and retrieve data. The `AzureTableStore` class is stateless, which means that it does not need to maintain any state across nodes. This makes it resilient to node failures, since it does not rely on maintaining state across nodes. It also makes it easy to configure, since it does not require setting up additional infrastructure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Custom Rendezvous Backend\n",
    "\n",
    "The custom rendezvous backend is implemented in the ``src/azure_table_backend.py`` file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Custom Rendezvous Handler\n",
    "\n",
    "To allow PyTorch Elastic to use Azure Data Table as the rendezvous store for distributed training, we can create an instance of the `AzureTableRendezvousBackend` class and pass it to the `RendezvousHandler` object. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Distributed Elastic Training with PyTorch\n",
    "\n",
    "Distributed elastic training allows us to dynamically adjust the number of training workers during the training process. In this section, we will implement the PyTorch distributed elastic training code and integrate our custom PyTorch Store and Rendezvous Backend."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submit the job to Azure Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Configure Command for reading and writing data\n",
    "The CIFAR 10 dataset, a compressed file,  is downloaded from a public url. The `read_write_data.py` code which is in the `src` folder does the extraction of files using the `tarfile library`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading src (0.02 MBs): 100%|##########| 21798/21798 [00:00<00:00, 70059.55it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/sincere_sun_wvznthjf13?wsid=/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/gasi_rg_centraleuap/workspaces/sdk_vnext_cli&tid=72f988bf-86f1-41af-91ab-2d7cd011db47'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    \"cifar_zip\": Input(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "outputs = {\n",
    "    \"cifar\": Output(\n",
    "        type=AssetTypes.URI_FOLDER,\n",
    "        path=f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/workspaceblobstore/paths/CIFAR-10\",\n",
    "    )\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_write_data.py --input_data ${{inputs.cifar_zip}} --output_folder ${{outputs.cifar}}\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.studio_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: sincere_sun_wvznthjf13\n",
      "Web View: https://ml.azure.com/runs/sincere_sun_wvznthjf13?wsid=/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/gasi_rg_centraleuap/workspaces/sdk_vnext_cli\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: sincere_sun_wvznthjf13\n",
      "Web View: https://ml.azure.com/runs/sincere_sun_wvznthjf13?wsid=/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/gasi_rg_centraleuap/workspaces/sdk_vnext_cli\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sincere_sun_wvznthjf13\n",
      "elastic\n",
      "${{parent.jobs.sincere_sun_wvznthjf13.outputs.cifar}}\n",
      "azureml://subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/gasi_rg_centraleuap/workspaces/sdk_vnext_cli/datastores/workspaceblobstore/paths/CIFAR-10\n"
     ]
    }
   ],
   "source": [
    "print(returned_job.name)\n",
    "print(returned_job.experiment_name)\n",
    "print(returned_job.outputs.cifar)\n",
    "print(returned_job.outputs.cifar.path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Configure Command Job for Distributed Training with PyTorch Elastic\n",
    "\n",
    "In this section, we will configure the command job for distributed training with PyTorch Elastic. Using the `max_instance_count` parameter, we can specify how much we are allowed to scale our cluster up to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# === Note on path ===\n",
    "# can be can be a local path or a cloud path. AzureML supports https://`, `abfss://`, `wasbs://` and `azureml://` URIs.\n",
    "# Local paths are automatically uploaded to the default datastore in the cloud.\n",
    "# More details on supported paths: https://docs.microsoft.com/azure/machine-learning/how-to-read-write-data-v2#supported-paths\n",
    "\n",
    "inputs = {\n",
    "    \"cifar\": Input(\n",
    "        type=AssetTypes.URI_FOLDER, path=returned_job.outputs.cifar.path\n",
    "    ),  # path=\"azureml:azureml_stoic_cartoon_wgb3lgvgky_output_data_cifar:1\"), #path=\"azureml://datastores/workspaceblobstore/paths/azureml/stoic_cartoon_wgb3lgvgky/cifar/\"),\n",
    "    \"epoch\": 1,\n",
    "    \"batchsize\": 256,\n",
    "    \"lr\": 0.01,\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python driver.py --data-dir ${{inputs.cifar}} --epochs ${{inputs.epoch}} --batch-size ${{inputs.batchsize}} --learning-rate ${{inputs.lr}}\",\n",
    "    inputs=inputs,\n",
    "    environment=\"azureml:AzureML-ACPT-pytorch-1.11-py38-cuda11.3-gpu:7\",\n",
    "    # compute=compute_name,\n",
    "    compute=\"cpu-cluster\",\n",
    "    instance_count=3,\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        \"process_count_per_instance\": 1,\n",
    "        \"max_instance_count\": 3,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/sincere_sun_wvznthjf13?wsid=/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/gasi_rg_centraleuap/workspaces/sdk_vnext_cli&tid=72f988bf-86f1-41af-91ab-2d7cd011db47'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit the job\n",
    "ml_client.jobs.create_or_update(job)\n",
    "\n",
    "# Monitor the job on Azure ML Studio\n",
    "returned_job.studio_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchelastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
