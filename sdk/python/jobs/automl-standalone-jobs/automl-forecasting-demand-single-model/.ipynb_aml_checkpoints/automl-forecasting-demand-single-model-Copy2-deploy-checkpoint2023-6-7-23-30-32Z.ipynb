{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "**Demand Forecasting Using Many Models**\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Compute](#Compute)\n",
    "1. [Data](#Data)\n",
    "1. [Import Components From Registry](#ImportComponents)\n",
    "1. [Create a Pipeline](#Pipeline)\n",
    "1. [Kick Off Pipeline Runs](#PipelineRuns)\n",
    "1. [Download Output](#DownloadOutput)\n",
    "1. [Compare Evaluation Results](#CompareResults)\n",
    "1. [Deployment](#Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The objective of this notebook is to illustrate how to use the component-based AutoML hierarchical time series solution for demand forecasting tasks. It walks you through all stages of model evaluation and production process starting with data ingestion and concluding with batch endpoint deployment for production. Please see the following [link](placeholder) for a detailed description of the hierarchical time series modeling.\n",
    "\n",
    "We use a subset of UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)) with the objective of predicting electricity demand per consumer 24 hours ahead. The data was preprocessed using the [data prep notebook](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-data-preparation/auto-ml-forecasting-data-preparation.ipynb). Please refer to it for illustration on how to download the data from the source, aggregate to an hourly frequency, convert from wide to long format and upload to the Datastore. Here, we will work with the data that has been pre-processed and saved locally in the parquet format.\n",
    "\n",
    "There are a number of steps you need to take before you can put a model into production. A user needs to prepare the data, partition it into appropriate sets, select the best model, evaluate it against a baseline, and monitor the model in real life to collect enough observations on how it would perform had it been put in production. Some of these steps are time consuming, some require certain expertise in writing code. The steps shown in this notebook follow a typical thought process one follows before the model is put in production.\n",
    "\n",
    "Make sure you have executed the [configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1682992408484
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import yaml\n",
    "import azure.ai.ml\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import (\n",
    "    BatchEndpoint,\n",
    "    BatchDeployment,\n",
    "    AmlCompute,\n",
    "    PipelineComponentBatchDeployment,\n",
    ")\n",
    "from azure.ai.ml.entities._job.automl.tabular.forecasting_settings import (\n",
    "    ForecastingSettings,\n",
    ")\n",
    "\n",
    "print(f\"SDK version: {azure.ai.ml.__version__}\")\n",
    "os.environ[\n",
    "    \"AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED\"\n",
    "] = \"true\"  # TODO: Delete once CE is released"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential does not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "    print(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscription_id = \"381b38e9-9840-4719-a5a0-61d9585e1e91\"\n",
    "# resource_group = \"vlbejan_eastus2_rg\"\n",
    "# workspace = \"vlbejan_cuseuap\"\n",
    "\n",
    "# ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "# print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Workspace</th>\n",
       "      <td>vlbejan_eastus2_new_ws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subscription ID</th>\n",
       "      <td>381b38e9-9840-4719-a5a0-61d9585e1e91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource Group</th>\n",
       "      <td>vlbejan_eastus2_rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>eastus2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     \n",
       "Workspace                      vlbejan_eastus2_new_ws\n",
       "Subscription ID  381b38e9-9840-4719-a5a0-61d9585e1e91\n",
       "Resource Group                     vlbejan_eastus2_rg\n",
       "Location                                      eastus2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.connections._subscription_id\n",
    "output[\"Resource Group\"] = ws.resource_group\n",
    "output[\"Location\"] = ws.location\n",
    "pd.DataFrame(data=output, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute \n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "\n",
    "You will need to create a compute target for your AutoML run. In this tutorial, you will create AmlCompute as your training compute resource.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "\n",
    "Here, we use a 5 node cluster of the `STANDARD_DS15_V2` series for illustration purposes. You will need to adjust the compute type and the number of nodes based on your needs which can be driven by the speed needed for model selection, data size, etc. \n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "amlcompute_cluster_name = \"demand-fcst-hts-cluster\"  # \"nc6s-v3-10nodes-low-priority\" # \"demand-fcst-single-cluster\"  # TODO: change to appropriate cluster after testing\n",
    "\n",
    "try:\n",
    "    # Retrieve an already attached Azure Machine Learning Compute.\n",
    "    compute_target = ml_client.compute.get(amlcompute_cluster_name)\n",
    "except ResourceNotFoundError as e:\n",
    "    compute_target = AmlCompute(\n",
    "        name=amlcompute_cluster_name,\n",
    "        size=\"STANDARD_NC6\",\n",
    "        type=\"amlcompute\",\n",
    "        min_instances=0,\n",
    "        max_instances=10,\n",
    "        idle_time_before_scale_down=600,\n",
    "    )\n",
    "    poller = ml_client.begin_create_or_update(amlcompute_cluster_name)\n",
    "    poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data\n",
    "\n",
    "For illustration purposes we use the UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)). The original dataset contains electricity consumption data for 370 consumers measured at 15 minute intervals. In the data set for this demonstrations, we have aggregated to an hourly frequency and convereted to the kilowatt hours (kWh) for 10 customers. Each customer is assigned to one of the two groups as denoted by the entries in the `group_id` column. The following cells read and print the first few rows of the training data as well as print the number of unique time series in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = \"datetime\"\n",
    "target_column_name = \"usage\"\n",
    "time_series_id_column_names = [\"group_id\", \"customer_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>usage</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MT_023</td>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "      <td>7.915567</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MT_023</td>\n",
       "      <td>2012-01-01 01:00:00</td>\n",
       "      <td>9.234828</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MT_023</td>\n",
       "      <td>2012-01-01 02:00:00</td>\n",
       "      <td>8.905013</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id            datetime     usage group_id\n",
       "0      MT_023 2012-01-01 00:00:00  7.915567        A\n",
       "1      MT_023 2012-01-01 01:00:00  9.234828        A\n",
       "2      MT_023 2012-01-01 02:00:00  8.905013        A"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_type = \"train\"\n",
    "df = pd.read_parquet(f\"./data/{dataset_type}/uci_electro_small_{dataset_type}.parquet\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 10 individual time-series\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nseries = df.groupby(time_series_id_column_names).ngroups\n",
    "print(f\"Data contains {nseries} individual time-series\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>MT_023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25945</th>\n",
       "      <td>A</td>\n",
       "      <td>MT_070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51890</th>\n",
       "      <td>A</td>\n",
       "      <td>MT_103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77835</th>\n",
       "      <td>A</td>\n",
       "      <td>MT_147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103780</th>\n",
       "      <td>A</td>\n",
       "      <td>MT_148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129725</th>\n",
       "      <td>B</td>\n",
       "      <td>MT_210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155670</th>\n",
       "      <td>B</td>\n",
       "      <td>MT_235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181615</th>\n",
       "      <td>B</td>\n",
       "      <td>MT_315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207560</th>\n",
       "      <td>B</td>\n",
       "      <td>MT_345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233505</th>\n",
       "      <td>B</td>\n",
       "      <td>MT_355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       group_id customer_id\n",
       "0             A      MT_023\n",
       "25945         A      MT_070\n",
       "51890         A      MT_103\n",
       "77835         A      MT_147\n",
       "103780        A      MT_148\n",
       "129725        B      MT_210\n",
       "155670        B      MT_235\n",
       "181615        B      MT_315\n",
       "207560        B      MT_345\n",
       "233505        B      MT_355"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[time_series_id_column_names].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLTable defined locally, with local data to be uploaded\n",
    "train_dataset = Input(type=AssetTypes.MLTABLE, path=\"./data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we will use our test data set from the pipeline run and we will need to upload it to URI directory to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Input(type=AssetTypes.URI_FOLDER, path=\"./data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import Components From Registry\n",
    "\n",
    "An Azure Machine Learning component is a self-contained piece of code that does one step in a machine learning pipeline. A component is analogous to a function - it has a name, inputs, outputs, and a body. Components are the building blocks of the Azure Machine Learning pipelines. It's a good engineering practice to build a machine learning pipeline where each step has well-defined inputs and outputs. In Azure Machine Learning, a component represents one reusable step in a pipeline. Components are designed to help improve the productivity of pipeline building. Specifically, components offer:\n",
    "- Well-defined interface: Components require a well-defined interface (input and output). The interface allows the user to build steps and connect steps easily. The interface also hides the complex logic of a step and removes the burden of understanding how the step is implemented.\n",
    "\n",
    "- Share and reuse: As the building blocks of a pipeline, components can be easily shared and reused across pipelines, workspaces, and subscriptions. Components built by one team can be discovered and used by another team.\n",
    "\n",
    "- Version control: Components are versioned. The component producers can keep improving components and publish new versions. Consumers can use specific component versions in their pipelines. This gives them compatibility and reproducibility.\n",
    "\n",
    "For a more detailed information on this subject, refer to the this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2).\n",
    "\n",
    "To import components,  we need to get the registry. The following command obtains the public regsitry from which we will import components for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682992409837
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f6b884a0190>,\n",
      "         subscription_id=6c6683e9-e5fe-4038-8519-ce6ebec2ba15,\n",
      "         resource_group_name=registry-builtin-staging-eastus,\n",
      "         workspace_name=None)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# get registry\n",
    "# TODO: Change registry name once the components are in the public registry\n",
    "ml_client_registry = MLClient(\n",
    "    credential=credential, registry_name=\"azureml-staging\"\n",
    ")  # \"ManyModels_HTS_BugBash\")\n",
    "print(ml_client_registry)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f6b884a0190>,\n",
      "         subscription_id=381b38e9-9840-4719-a5a0-61d9585e1e91,\n",
      "         resource_group_name=yuzhua-rg-eastasia,\n",
      "         workspace_name=None)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# get registry\n",
    "# TODO: Change registry name once the components are in the public registry\n",
    "ml_client_registry_metrics = MLClient(\n",
    "    credential=credential, registry_name=\"ManyModels_HTS_BugBash\"\n",
    ")  # \"ManyModels_HTS_BugBash\")\n",
    "print(ml_client_registry_metrics)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pull specific components and use them to build a pipeline of steps. For the illustration of the product evaluation workflow we will use the following components:\n",
    "- Data partitioning component: allows users to partion the data for many models runs, both, training and inference.\n",
    "- Many models training component: trains the best model per partition specified by users.\n",
    "- Many moodels inference componnet: generates forecast for each partition. This can be done on the test and inference sets.\n",
    "- Compute metrics component: calculates metrics per time series if the inference component was used on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996405805
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many models inference component version: 0.0.1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "inference_component = ml_client_registry.components.get(\n",
    "    name=\"automl_forecasting_inference\", label=\"latest\"\n",
    ")\n",
    "print(f\"Many models inference component version: {inference_component.version}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996406713
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many models inference component version: 0.0.18.preview\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_component = ml_client_registry_metrics.components.get(\n",
    "    name=\"compute_metrics\", label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Many models inference component version: {compute_metrics_component.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1682996406974
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## 6. Create a Pipeline\n",
    "\n",
    "Now that we imported the components we will build an evaluation pipeline. This pipeline will allow us to partition the data, train best models for each partition, genererate rolling forecasts on the test set, and, finally, calculate metrics on the test set output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Create a YML\n",
    "\n",
    "AzureML components can only receive specific object types such as strings, JSON/YML files, URI Folders and URI Files. Other object types are not accepted. Because of this, the settings needs to be passed into the training component in YML format.\n",
    "\n",
    "The following are the bare-minimum parameters needed to successfully train many models. For a finer control of the experiment a user may add other parameters to the config file. See the [forecast settings API doc](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.automl.forecastingjob#azure-ai-ml-automl-forecastingjob-set-forecast-settings) for a complete list of available parameters. \n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **task**                           | forecasting |\n",
    "| **primary_metric**                 | This is the metric that you want to optimize. Forecasting supports the following primary metrics<ul><li>`normalized_root_mean_squared_error`</li><li>`normalized_mean_absolute_error`</li><li>`spearman_correlation`</li><li>`r2_score`</li></ul> We recommend using either the normalized root mean squared error or normalized mean absolute erorr as a primary metric because they measure forecast accuracy. See the [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-automl-forecasting-faq#how-do-i-choose-the-primary-metric) for a more detailed discussion on this topic. |\n",
    "| **forecast_horizon**       | The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly). |\n",
    "| **label_column_name**      | The name of the target column we are trying to predict. |\n",
    "| **time_column_name**       | The name of your time column. |\n",
    "| **hierarchy_column_names** | The names of columns that define the hierarchical structure of the data from highest level to most granular. |\n",
    "| **enable_early_stopping**  | Flag to enable early termination if the primary metric is no longer improving. |\n",
    "| **partition_column_names** | The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series. |\n",
    "| **allow_multi_partitions** | A flag that allows users to train one model per partition when each partition contians more than one unique time series. The dafault value is `False`. |\n",
    "| **track_child_runs**       | Flag to disable tracking of child runs. Only best run is tracked if the flag is set to False (this includes the model and metrics of the run). |\n",
    "| **enable_early_stopping**  | Flag to enable early termination if the primary metric is no longer improving. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = dict(\n",
    "    task=\"forecasting\",\n",
    "    target_column_name=target_column_name,\n",
    "    primary_metric=\"normalized_root_mean_squared_error\",\n",
    "    n_cross_validations=\"auto\",\n",
    "    enable_model_explainability=True,\n",
    "    validation_data=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_settings = dict(\n",
    "    forecast_horizon=24,\n",
    "    time_column_name=time_column_name,\n",
    "    time_series_id_column_names=time_series_id_column_names,\n",
    "    cv_step_size=24,\n",
    "    # target_lags = None,\n",
    "    # target_rolling_window_size = None,\n",
    "    # frequency = None\n",
    "    # feature_lags = None,\n",
    "    # seasonality = None,\n",
    "    # use_stl = None,\n",
    "    # short_series_handling_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_settings = dict(\n",
    "    enable_dnn_training=False,  # True,\n",
    "    training_mode=None,  # \"distributed\",\n",
    "    allowed_training_algorithms=[\"LightGBM\"],  # [\"TCNForecaster\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_limits = dict(\n",
    "    max_concurrent_trials=2,  # 5,\n",
    "    max_cores_per_trial=-1,\n",
    "    # max_nodes = 10, # 10,\n",
    "    max_trials=5,  # 20,\n",
    "    timeout_minutes=90,\n",
    "    trial_timeout_minutes=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_settings = dict(\n",
    "    compute_name=amlcompute_cluster_name,\n",
    "    forecast_mode=\"rolling\",\n",
    "    forecast_step=24,\n",
    "    is_validation_data_provided=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"task\": \"forecasting\",\n",
      "    \"target_column_name\": \"usage\",\n",
      "    \"primary_metric\": \"normalized_root_mean_squared_error\",\n",
      "    \"n_cross_validations\": \"auto\",\n",
      "    \"enable_model_explainability\": true,\n",
      "    \"validation_data\": null,\n",
      "    \"forecast_horizon\": 24,\n",
      "    \"time_column_name\": \"datetime\",\n",
      "    \"time_series_id_column_names\": [\n",
      "        \"group_id\",\n",
      "        \"customer_id\"\n",
      "    ],\n",
      "    \"cv_step_size\": 24,\n",
      "    \"enable_dnn_training\": false,\n",
      "    \"training_mode\": null,\n",
      "    \"allowed_training_algorithms\": [\n",
      "        \"LightGBM\"\n",
      "    ],\n",
      "    \"max_concurrent_trials\": 2,\n",
      "    \"max_cores_per_trial\": -1,\n",
      "    \"max_trials\": 5,\n",
      "    \"timeout_minutes\": 90,\n",
      "    \"trial_timeout_minutes\": 60,\n",
      "    \"compute_name\": \"demand-fcst-hts-cluster\",\n",
      "    \"forecast_mode\": \"rolling\",\n",
      "    \"forecast_step\": 24,\n",
      "    \"is_validation_data_provided\": false\n",
      "} \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "pipeline_parameters = {\n",
    "    **automl_settings,\n",
    "    **forecast_settings,\n",
    "    **training_settings,\n",
    "    **training_limits,\n",
    "    **component_settings,\n",
    "}\n",
    "print(json.dumps(pipeline_parameters, indent=4), \"\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Provide additional pipeline parameters\n",
    "\n",
    "The next set of parameters is necessary to build the pipeline of components. These parameters are specific to the many models training and/or inference components. Since both of these components rely on the Parallel run step (PRS) to train/inference multiple models at once, you will need to determine the appropriate number of workers and nodes for your use case. The `max_concurrency_per_instance` is based off the number of cores of the compute VM. The `max_nodes` will determine the number of nodes to use, increasing the node count will speed up the training process.\n",
    "\n",
    "|Property|Description|\n",
    "|:-|:-|\n",
    "| **max_nodes**                     | The number of compute nodes in a cluster to be used for training and inferencing steps. We recommend to start with 3 and increase the node_count if the training time is taking too long. |\n",
    "| **max_concurrency_per_instance**   | Process count per node. We recommend a 2:1 ratio for number of cores to the number of processes per node. For example, if a node has 16 cores then configure 8 **or less** process counts per node for optimal performance. |\n",
    "|**retrain_failed_model**| If training a model for any partition fails, should AutoML kick off a new child run for that partition? Possible values are `True` or `False`.|\n",
    "|**forecast_mode**| Type of forecat to perform on the test set. Can be `recursive` or `rolling`. Rolling forecast can be used for the evaluation purposes |\n",
    "| **prs_timeout_seconds**         | Maximum amount of time in seconds that the `ParallelRunStep` class is allowed. This is optional but provides customers with greater control on exit criteria. This must be greater than `experiment_timeout_hours` by at least 300 seconds. |\n",
    "|**partition_column_names**| The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series. This parameter is identical to the one in the `automl_config` object.|\n",
    "|**compute_name**| Name of the compute to execute the pipeline on. |\n",
    "|**enable_event_logger**| Set this value to `True` to enable event logger. |\n",
    "| **input_type**               | Type of file format for the input data. Supported options are `csv` and `parquet`. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Build a Pipeline\n",
    "\n",
    "Next, we build a pipeline from the imported components. Since this notebook is designed to illustrate the evaluation flow, we will string these components in the following fashion. First, we train the best model for each partition. Then, we generate a rolling forecast with the step size of 24 (hours) on the test set. This is done to mimic the evaluation process when a customer is tracking model's performance in real time and generates forecasts every 24 hours. Finally, we compute metrics based on the rolling forecast output from the previous step. You do not have to modify anything in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForecastingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_forecast_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_column_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforecast_horizon\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_series_id_column_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_lags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeature_lags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_rolling_window_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcountry_or_region_for_holidays\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_stl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseasonality\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshort_series_handling_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfrequency\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_aggregate_function\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcv_step_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mset_forecast_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtime_column_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mforecast_horizon\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtime_series_id_column_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtarget_lags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mfeature_lags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtarget_rolling_window_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcountry_or_region_for_holidays\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0muse_stl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mseasonality\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mshort_series_handling_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mfrequency\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtarget_aggregate_function\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcv_step_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Manage parameters used by forecasting tasks.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :param time_column_name:\u001b[0m\n",
       "\u001b[0;34m            The name of the time column. This parameter is required when forecasting to specify the datetime\u001b[0m\n",
       "\u001b[0;34m            column in the input data used for building the time series and inferring its frequency.\u001b[0m\n",
       "\u001b[0;34m        :type time_column_name: str\u001b[0m\n",
       "\u001b[0;34m        :param forecast_horizon:\u001b[0m\n",
       "\u001b[0;34m            The desired maximum forecast horizon in units of time-series frequency. The default value is 1.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            Units are based on the time interval of your training data, e.g., monthly, weekly that the forecaster\u001b[0m\n",
       "\u001b[0;34m            should predict out. When task type is forecasting, this parameter is required. For more information on\u001b[0m\n",
       "\u001b[0;34m            setting forecasting parameters, see `Auto-train a time-series forecast model <https://docs.microsoft.com/\u001b[0m\n",
       "\u001b[0;34m            azure/machine-learning/how-to-auto-train-forecast>`_.\u001b[0m\n",
       "\u001b[0;34m        :type forecast_horizon: int or str\u001b[0m\n",
       "\u001b[0;34m        :param time_series_id_column_names:\u001b[0m\n",
       "\u001b[0;34m            The names of columns used to group a timeseries.\u001b[0m\n",
       "\u001b[0;34m            It can be used to create multiple series. If time series id column names is not defined or\u001b[0m\n",
       "\u001b[0;34m            the identifier columns specified do not identify all the series in the dataset, the time series identifiers\u001b[0m\n",
       "\u001b[0;34m            will be automatically created for your dataset.\u001b[0m\n",
       "\u001b[0;34m        :type time_series_id_column_names: str or list(str)\u001b[0m\n",
       "\u001b[0;34m        :param target_lags:\u001b[0m\n",
       "\u001b[0;34m            The number of past periods to lag from the target column. By default the lags are turned off.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            When forecasting, this parameter represents the number of rows to lag the target values based\u001b[0m\n",
       "\u001b[0;34m            on the frequency of the data. This is represented as a list or single integer. Lag should be used\u001b[0m\n",
       "\u001b[0;34m            when the relationship between the independent variables and dependent variable do not match up or\u001b[0m\n",
       "\u001b[0;34m            correlate by default. For example, when trying to forecast demand for a product, the demand in any\u001b[0m\n",
       "\u001b[0;34m            month may depend on the price of specific commodities 3 months prior. In this example, you may want\u001b[0m\n",
       "\u001b[0;34m            to lag the target (demand) negatively by 3 months so that the model is training on the correct\u001b[0m\n",
       "\u001b[0;34m            relationship. For more information, see `Auto-train a time-series forecast model\u001b[0m\n",
       "\u001b[0;34m            <https://docs.microsoft.com/azure/machine-learning/how-to-auto-train-forecast>`_.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            **Note on auto detection of target lags and rolling window size.\u001b[0m\n",
       "\u001b[0;34m            Please see the corresponding comments in the rolling window section.**\u001b[0m\n",
       "\u001b[0;34m            We use the next algorithm to detect the optimal target lag and rolling window size.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            #. Estimate the maximum lag order for the look back feature selection. In our case it is the number of\u001b[0m\n",
       "\u001b[0;34m               periods till the next date frequency granularity i.e. if frequency is daily, it will be a week (7),\u001b[0m\n",
       "\u001b[0;34m               if it is a week, it will be month (4). That values multiplied by two is the largest\u001b[0m\n",
       "\u001b[0;34m               possible values of lags/rolling windows. In our examples, we will consider the maximum lag\u001b[0m\n",
       "\u001b[0;34m               order of 14 and 8 respectively).\u001b[0m\n",
       "\u001b[0;34m            #. Create a de-seasonalized series by adding trend and residual components. This will be used\u001b[0m\n",
       "\u001b[0;34m               in the next step.\u001b[0m\n",
       "\u001b[0;34m            #. Estimate the PACF - Partial Auto Correlation Function on the on the data from (2)\u001b[0m\n",
       "\u001b[0;34m               and search for points, where the auto correlation is significant i.e. its absolute\u001b[0m\n",
       "\u001b[0;34m               value is more then 1.96/square_root(maximal lag value), which correspond to significance of 95%.\u001b[0m\n",
       "\u001b[0;34m            #. If all points are significant, we consider it being strong seasonality\u001b[0m\n",
       "\u001b[0;34m               and do not create look back features.\u001b[0m\n",
       "\u001b[0;34m            #. We scan the PACF values from the beginning and the value before the first insignificant\u001b[0m\n",
       "\u001b[0;34m               auto correlation will designate the lag. If first significant element (value correlate with\u001b[0m\n",
       "\u001b[0;34m               itself) is followed by insignificant, the lag will be 0 and we will not use look back features.\u001b[0m\n",
       "\u001b[0;34m        :type target_lags: int, str, or list(int)\u001b[0m\n",
       "\u001b[0;34m        :param feature_lags: Flag for generating lags for the numeric features with 'auto' or None.\u001b[0m\n",
       "\u001b[0;34m        :type feature_lags: str or None\u001b[0m\n",
       "\u001b[0;34m        :param target_rolling_window_size:\u001b[0m\n",
       "\u001b[0;34m            The number of past periods used to create a rolling window average of the target column.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            When forecasting, this parameter represents `n` historical periods to use to generate forecasted values,\u001b[0m\n",
       "\u001b[0;34m            <= training set size. If omitted, `n` is the full training set size. Specify this parameter\u001b[0m\n",
       "\u001b[0;34m            when you only want to consider a certain amount of history when training the model.\u001b[0m\n",
       "\u001b[0;34m            If set to 'auto', rolling window will be estimated as the last\u001b[0m\n",
       "\u001b[0;34m            value where the PACF is more then the significance threshold. Please see target_lags section for details.\u001b[0m\n",
       "\u001b[0;34m        :type target_rolling_window_size: int, str or None\u001b[0m\n",
       "\u001b[0;34m        :param country_or_region_for_holidays: The country/region used to generate holiday features.\u001b[0m\n",
       "\u001b[0;34m            These should be ISO 3166 two-letter country/region codes, for example 'US' or 'GB'.\u001b[0m\n",
       "\u001b[0;34m        :type country_or_region_for_holidays: str or None\u001b[0m\n",
       "\u001b[0;34m        :param use_stl: Configure STL Decomposition of the time-series target column.\u001b[0m\n",
       "\u001b[0;34m                    use_stl can take three values: None (default) - no stl decomposition, 'season' - only generate\u001b[0m\n",
       "\u001b[0;34m                    season component and season_trend - generate both season and trend components.\u001b[0m\n",
       "\u001b[0;34m        :type use_stl: str or None\u001b[0m\n",
       "\u001b[0;34m        :param seasonality: Set time series seasonality as an integer multiple of the series frequency.\u001b[0m\n",
       "\u001b[0;34m                    If seasonality is set to 'auto', it will be inferred.\u001b[0m\n",
       "\u001b[0;34m                    If set to None, the time series is assumed non-seasonal which is equivalent to seasonality=1.\u001b[0m\n",
       "\u001b[0;34m        :type seasonality: int, str or None\u001b[0m\n",
       "\u001b[0;34m        :param short_series_handling_config:\u001b[0m\n",
       "\u001b[0;34m            The parameter defining how if AutoML should handle short time series.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            Possible values: 'auto' (default), 'pad', 'drop' and None.\u001b[0m\n",
       "\u001b[0;34m            * **auto** short series will be padded if there are no long series,\u001b[0m\n",
       "\u001b[0;34m            otherwise short series will be dropped.\u001b[0m\n",
       "\u001b[0;34m            * **pad** all the short series will be padded.\u001b[0m\n",
       "\u001b[0;34m            * **drop**  all the short series will be dropped\".\u001b[0m\n",
       "\u001b[0;34m            * **None** the short series will not be modified.\u001b[0m\n",
       "\u001b[0;34m            If set to 'pad', the table will be padded with the zeroes and\u001b[0m\n",
       "\u001b[0;34m            empty values for the regressors and random values for target with the mean\u001b[0m\n",
       "\u001b[0;34m            equal to target value median for given time series id. If median is more or equal\u001b[0m\n",
       "\u001b[0;34m            to zero, the minimal padded value will be clipped by zero:\u001b[0m\n",
       "\u001b[0;34m            Input:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            +------------+---------------+----------+--------+\u001b[0m\n",
       "\u001b[0;34m            | Date       | numeric_value | string   | target |\u001b[0m\n",
       "\u001b[0;34m            +============+===============+==========+========+\u001b[0m\n",
       "\u001b[0;34m            | 2020-01-01 | 23            | green    | 55     |\u001b[0m\n",
       "\u001b[0;34m            +------------+---------------+----------+--------+\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            Output assuming minimal number of values is four:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            +------------+---------------+----------+--------+\u001b[0m\n",
       "\u001b[0;34m            | Date       | numeric_value | string   | target |\u001b[0m\n",
       "\u001b[0;34m            +============+===============+==========+========+\u001b[0m\n",
       "\u001b[0;34m            | 2019-12-29 | 0             | NA       | 55.1   |\u001b[0m\n",
       "\u001b[0;34m            +------------+---------------+----------+--------+\u001b[0m\n",
       "\u001b[0;34m            | 2019-12-30 | 0             | NA       | 55.6   |\u001b[0m\n",
       "\u001b[0;34m            +------------+---------------+----------+--------+\u001b[0m\n",
       "\u001b[0;34m            | 2019-12-31 | 0             | NA       | 54.5   |\u001b[0m\n",
       "\u001b[0;34m            +------------+---------------+----------+--------+\u001b[0m\n",
       "\u001b[0;34m            | 2020-01-01 | 23            | green    | 55     |\u001b[0m\n",
       "\u001b[0;34m            +------------+---------------+----------+--------+\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            **Note:** We have two parameters short_series_handling_configuration and\u001b[0m\n",
       "\u001b[0;34m            legacy short_series_handling. When both parameters are set we are\u001b[0m\n",
       "\u001b[0;34m            synchronize them as shown in the table below (short_series_handling_configuration and\u001b[0m\n",
       "\u001b[0;34m            short_series_handling for brevity are marked as handling_configuration and handling\u001b[0m\n",
       "\u001b[0;34m            respectively).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            |  handling | handling_configuration | resulting handling | resulting handling_configuration |\u001b[0m\n",
       "\u001b[0;34m            +===========+========================+====================+==================================+\u001b[0m\n",
       "\u001b[0;34m            | True      | auto                   | True               | auto                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            | True      | pad                    | True               | auto                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            | True      | drop                   | True               | auto                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            | True      | None                   | False              | None                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            | False     | auto                   | False              | None                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            | False     | pad                    | False              | None                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            | False     | drop                   | False              | None                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m            | False     | None                   | False              | None                             |\u001b[0m\n",
       "\u001b[0;34m            +-----------+------------------------+--------------------+----------------------------------+\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :type short_series_handling_config: str or None\u001b[0m\n",
       "\u001b[0;34m        :param frequency: Forecast frequency.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            When forecasting, this parameter represents the period with which the forecast is desired,\u001b[0m\n",
       "\u001b[0;34m            for example daily, weekly, yearly, etc. The forecast frequency is dataset frequency by default.\u001b[0m\n",
       "\u001b[0;34m            You can optionally set it to greater (but not lesser) than dataset frequency.\u001b[0m\n",
       "\u001b[0;34m            We'll aggregate the data and generate the results at forecast frequency. For example,\u001b[0m\n",
       "\u001b[0;34m            for daily data, you can set the frequency to be daily, weekly or monthly, but not hourly.\u001b[0m\n",
       "\u001b[0;34m            The frequency needs to be a pandas offset alias.\u001b[0m\n",
       "\u001b[0;34m            Please refer to pandas documentation for more information:\u001b[0m\n",
       "\u001b[0;34m            https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\u001b[0m\n",
       "\u001b[0;34m        :type frequency: str or None\u001b[0m\n",
       "\u001b[0;34m        :param target_aggregate_function: The function to be used to aggregate the time series target\u001b[0m\n",
       "\u001b[0;34m                                            column to conform to a user specified frequency. If the\u001b[0m\n",
       "\u001b[0;34m                                            target_aggregation_function is set, but the freq parameter\u001b[0m\n",
       "\u001b[0;34m                                            is not set, the error is raised. The possible target\u001b[0m\n",
       "\u001b[0;34m                                            aggregation functions are: \"sum\", \"max\", \"min\" and \"mean\".\u001b[0m\n",
       "\u001b[0;34m                * The target column values are aggregated based on the specified operation.\u001b[0m\n",
       "\u001b[0;34m                  Typically, sum is appropriate for most scenarios.\u001b[0m\n",
       "\u001b[0;34m                * Numerical predictor columns in your data are aggregated by sum, mean, minimum value,\u001b[0m\n",
       "\u001b[0;34m                  and maximum value. As a result, automated ML generates new columns suffixed with the\u001b[0m\n",
       "\u001b[0;34m                  aggregation function name and applies the selected aggregate operation.\u001b[0m\n",
       "\u001b[0;34m                * For categorical predictor columns, the data is aggregated by mode,\u001b[0m\n",
       "\u001b[0;34m                  the most prominent category in the window.\u001b[0m\n",
       "\u001b[0;34m                * Date predictor columns are aggregated by minimum value, maximum value and mode.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m                +----------------+-----------------------------+---------------------------------------------+\u001b[0m\n",
       "\u001b[0;34m                |      freq      | target_aggregation_function |      Data regularity fixing mechanism       |\u001b[0m\n",
       "\u001b[0;34m                +================+=============================+=============================================+\u001b[0m\n",
       "\u001b[0;34m                | None (Default) | None (Default)              | The aggregation is not applied.             |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | If the valid frequency can not be           |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | determined the error will be raised.        |\u001b[0m\n",
       "\u001b[0;34m                +----------------+-----------------------------+---------------------------------------------+\u001b[0m\n",
       "\u001b[0;34m                | Some Value     | None (Default)              | The aggregation is not applied.             |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | If the number of data points compliant      |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | to given frequency grid is less then 90%    |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | these points will be removed, otherwise     |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | the error will be raised.                   |\u001b[0m\n",
       "\u001b[0;34m                +----------------+-----------------------------+---------------------------------------------+\u001b[0m\n",
       "\u001b[0;34m                | None (Default) | Aggregation function        | The error about missing frequency parameter |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | is raised.                                  |\u001b[0m\n",
       "\u001b[0;34m                +----------------+-----------------------------+---------------------------------------------+\u001b[0m\n",
       "\u001b[0;34m                | Some Value     | Aggregation function        | Aggregate to frequency using provided       |\u001b[0m\n",
       "\u001b[0;34m                |                |                             | aggregation function.                       |\u001b[0m\n",
       "\u001b[0;34m                +----------------+-----------------------------+---------------------------------------------+\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :type target_aggregate_function: str or None\u001b[0m\n",
       "\u001b[0;34m        :param cv_step_size:\u001b[0m\n",
       "\u001b[0;34m            Number of periods between the origin_time of one CV fold and the next fold. For\u001b[0m\n",
       "\u001b[0;34m            example, if `n_step` = 3 for daily data, the origin time for each fold will be\u001b[0m\n",
       "\u001b[0;34m            three days apart.\u001b[0m\n",
       "\u001b[0;34m        :type cv_step_size: int or None\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mForecastingSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountry_or_region_for_holidays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcountry_or_region_for_holidays\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mcountry_or_region_for_holidays\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountry_or_region_for_holidays\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_step_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcv_step_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcv_step_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_step_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast_horizon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mforecast_horizon\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mforecast_horizon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast_horizon\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_lags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtarget_lags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget_lags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_lags\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_rolling_window_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtarget_rolling_window_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtarget_rolling_window_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_rolling_window_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfrequency\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_lags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfeature_lags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfeature_lags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_lags\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseasonality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mseasonality\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mseasonality\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseasonality\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_stl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_stl\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_stl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_stl\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_series_handling_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mshort_series_handling_config\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mshort_series_handling_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_series_handling_config\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_aggregate_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtarget_aggregate_function\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtarget_aggregate_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_aggregate_function\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_column_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtime_column_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtime_column_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_column_name\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_series_id_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtime_series_id_column_names\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtime_series_id_column_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecasting_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_series_id_column_names\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/entities/_job/automl/tabular/forecasting_job.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??automl.ForecastingJob.set_forecast_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group_id', 'customer_id']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_parameters.get(\"time_series_id_column_names\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1682996407118
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    description=\"AutoML Forecasting Single Model Evaluation Pipeline\",\n",
    ")\n",
    "def evaluation_pipeline(training_data, inference_data, validation_data=None):\n",
    "    # 0. Extract parameters from the dictionary\n",
    "    target_column_name = pipeline_parameters.get(\"target_column_name\")\n",
    "    primary_metric = pipeline_parameters.get(\n",
    "        \"primary_metric\", \"normalized_root_squared_error\"\n",
    "    )\n",
    "    n_cross_validations = pipeline_parameters.get(\"n_cross_validations\", \"auto\")\n",
    "\n",
    "    # -- 0.1 set_forecast_settings\n",
    "    time_column_name = pipeline_parameters.get(\"time_column_name\")\n",
    "    time_series_id_column_names = pipeline_parameters.get(\n",
    "        \"time_series_id_column_names\", None\n",
    "    )\n",
    "    country_or_region_for_holidays = pipeline_parameters.get(\n",
    "        \"country_or_region_for_holidays\", None\n",
    "    )\n",
    "    cv_step_size = pipeline_parameters.get(\"cv_step_size\", None)\n",
    "    forecast_horizon = pipeline_parameters.get(\"forecast_horizon\", None)\n",
    "    target_lags = pipeline_parameters.get(\"target_lags\", None)\n",
    "    target_rolling_window_size = pipeline_parameters.get(\n",
    "        \"target_rolling_window_size\", None\n",
    "    )\n",
    "    frequency = pipeline_parameters.get(\"frequency\", None)\n",
    "    feature_lags = pipeline_parameters.get(\"feature_lags\", None)\n",
    "    seasonality = pipeline_parameters.get(\"seasonality\", None)\n",
    "    use_stl = pipeline_parameters.get(\"use_stl\", None)\n",
    "    short_series_handling_config = pipeline_parameters.get(\n",
    "        \"short_series_handling_config\", None\n",
    "    )\n",
    "\n",
    "    # -- 0.2 set_training\n",
    "    enable_dnn_training = pipeline_parameters.get(\"enable_dnn_training\", False)\n",
    "    training_mode = pipeline_parameters.get(\"distributed\", None)\n",
    "    enable_model_explainability = pipeline_parameters.get(\"time_column_name\", True)\n",
    "    enable_stack_ensemble = pipeline_parameters.get(\"enable_stack_ensemble\", False)\n",
    "    enable_vote_ensemble = pipeline_parameters.get(\"enable_vote_ensemble\", True)\n",
    "    allowed_training_algorithms = pipeline_parameters.get(\n",
    "        \"allowed_training_algorithms\", None\n",
    "    )\n",
    "    blocked_training_algorithms = pipeline_parameters.get(\n",
    "        \"blocked_training_algorithms\", None\n",
    "    )\n",
    "\n",
    "    # -- 0.3 set_limits\n",
    "    max_concurrent_trials = pipeline_parameters.get(\"max_concurrent_trials\", None)\n",
    "    max_cores_per_trial = pipeline_parameters.get(\"max_cores_per_trial\", None)\n",
    "    max_nodes = pipeline_parameters.get(\"max_nodes\", None)\n",
    "    max_trials = pipeline_parameters.get(\"max_trials\", None)\n",
    "    timeout_minutes = pipeline_parameters.get(\"timeout_minutes\", None)\n",
    "    trial_timeout_minutes = pipeline_parameters.get(\"trial_timeout_minutes\", None)\n",
    "\n",
    "    # -- 0.4 component-specific settings\n",
    "    compute_name = pipeline_parameters.get(\"compute_name\")\n",
    "    max_nodes = pipeline_parameters.get(\"max_nodes\", 1)\n",
    "    forecast_mode = pipeline_parameters.get(\"forecast_mode\", \"recursive\")\n",
    "    forecast_step = pipeline_parameters.get(\"forecast_step\", 1)\n",
    "    forecast_quantiles = pipeline_parameters.get(\"forecast_quantiles\", None)\n",
    "    is_validation_data_provided = pipeline_parameters.get(\"forecast_quantiles\", False)\n",
    "\n",
    "    # 1. Model Training Step\n",
    "    # -- 1.1 Define the automl forecasting task with automl function\n",
    "    if is_validation_data_provided:\n",
    "        print(\"---\\n Wrong path \\n---\")\n",
    "        training_node = automl.forecasting(\n",
    "            compute=compute_name,\n",
    "            # experiment_name=experiment_name,\n",
    "            training_data=training_data,\n",
    "            validation_data=validation_data,\n",
    "            target_column_name=target_column_name,\n",
    "            primary_metric=primary_metric,\n",
    "            enable_model_explainability=enable_model_explainability,\n",
    "            outputs={\"best_model\": Output(type=AssetTypes.CUSTOM_MODEL)},\n",
    "        )\n",
    "    else:\n",
    "        training_node = automl.forecasting(\n",
    "            compute=compute_name,\n",
    "            # experiment_name=experiment_name,\n",
    "            training_data=training_data,\n",
    "            target_column_name=target_column_name,\n",
    "            primary_metric=primary_metric,\n",
    "            n_cross_validations=n_cross_validations,\n",
    "            enable_model_explainability=enable_model_explainability,\n",
    "            outputs={\"best_model\": Output(type=AssetTypes.CUSTOM_MODEL)},\n",
    "        )\n",
    "\n",
    "    # --  1.2 Define forecasting settings\n",
    "    training_node.set_forecast_settings(\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        time_column_name=time_column_name,\n",
    "        time_series_id_column_names=time_series_id_column_names,\n",
    "        country_or_region_for_holidays=country_or_region_for_holidays,\n",
    "        cv_step_size=cv_step_size,\n",
    "        target_lags=target_lags,\n",
    "        target_rolling_window_size=target_rolling_window_size,\n",
    "        frequency=frequency,\n",
    "        feature_lags=feature_lags,\n",
    "        seasonality=seasonality,\n",
    "        use_stl=use_stl,\n",
    "        short_series_handling_config=short_series_handling_config,\n",
    "    )\n",
    "\n",
    "    # -- 1.3 Set training parameters\n",
    "    training_node.set_training(\n",
    "        enable_dnn_training=enable_dnn_training,\n",
    "        training_mode=training_mode,\n",
    "        enable_model_explainability=enable_model_explainability,\n",
    "        allowed_training_algorithms=allowed_training_algorithms,\n",
    "    )\n",
    "\n",
    "    # -- 1.4 Set training limits. All limits are optional.\n",
    "    training_node.set_limits(\n",
    "        timeout_minutes=timeout_minutes,\n",
    "        trial_timeout_minutes=trial_timeout_minutes,\n",
    "        max_trials=max_trials,\n",
    "        max_concurrent_trials=max_concurrent_trials,\n",
    "        max_cores_per_trial=max_cores_per_trial,\n",
    "        max_nodes=max_nodes,\n",
    "    )\n",
    "\n",
    "    # 2. Inferencing step\n",
    "    inference_node = inference_component(\n",
    "        test_data=inference_data,\n",
    "        model_path=training_node.outputs.best_model,\n",
    "        target_column_name=target_column_name,\n",
    "        forecast_mode=forecast_mode,\n",
    "        forecast_step=forecast_step,\n",
    "        forecast_quantiles=forecast_quantiles,\n",
    "    )\n",
    "\n",
    "    # 3. Metrics calculation step\n",
    "    compute_metrics_node = compute_metrics_component(\n",
    "        task=\"tabular-forecasting\",\n",
    "        prediction=inference_node.outputs.inference_output_file,\n",
    "        ground_truth=inference_node.outputs.inference_output_file,\n",
    "        evaluation_config=inference_node.outputs.evaluation_config_output_file,\n",
    "    )\n",
    "    compute_metrics_node.compute = compute_name\n",
    "\n",
    "    # 4. Specify pipeline outputs\n",
    "    return {\n",
    "        \"output_files\": compute_metrics_node.outputs.evaluation_result,\n",
    "        \"output_model\": training_node.outputs.best_model,\n",
    "        \"forecast_output\": inference_node.outputs.inference_output_file,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Kick Off Pipeline Runs\n",
    "\n",
    "Now that the pipeline is defined, we will use it to kick off several runs. First, we will kick off an experiment which will train, inference and evaluate the performance for the best AutoML model for each partition. Next, we will kick off the same pipeline which will only use the naive model for the same partitions. This will allow us to establish a baseline and compare performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Kick Off Best Many Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ??azure.ai.ml.entities._inputs_outputs.input.Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ??azure.ai.ml.entities._job.pipeline._io.base.PipelineInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996407285
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name: evaluation_pipeline\n",
      "description: AutoML Forecasting Single Model Evaluation Pipeline\n",
      "type: pipeline\n",
      "inputs:\n",
      "  training_data:\n",
      "    type: uri_folder\n",
      "    path: azureml:./data/train\n",
      "  inference_data:\n",
      "    type: uri_folder\n",
      "    path: azureml:./data/test\n",
      "outputs:\n",
      "  output_files:\n",
      "    type: uri_folder\n",
      "  output_model:\n",
      "    type: custom_model\n",
      "  forecast_output:\n",
      "    type: uri_file\n",
      "jobs:\n",
      "  training_node:\n",
      "    name: training_node\n",
      "    type: automl\n",
      "    outputs:\n",
      "      best_model:\n",
      "        value: ${{parent.outputs.output_model}}\n",
      "        type: literal\n",
      "    limits:\n",
      "      max_concurrent_trials: 2\n",
      "      max_cores_per_trial: -1\n",
      "      max_nodes: 1\n",
      "      max_trials: 5\n",
      "      timeout_minutes: 90\n",
      "      trial_timeout_minutes: 60\n",
      "    training:\n",
      "      enable_dnn_training: false\n",
      "      enable_model_explainability: true\n",
      "      enable_stack_ensemble: false\n",
      "      allowed_training_algorithms:\n",
      "      - light_gbm\n",
      "    compute: azureml:demand-fcst-hts-cluster\n",
      "    target_column_name: usage\n",
      "    primary_metric: normalized_root_mean_squared_error\n",
      "    training_data: ${{parent.inputs.training_data}}\n",
      "    forecasting:\n",
      "      cv_step_size: 24\n",
      "      forecast_horizon: 24\n",
      "      time_column_name: datetime\n",
      "      time_series_id_column_names: '[''group_id'', ''customer_id'']'\n",
      "    n_cross_validations: auto\n",
      "    log_verbosity: info\n",
      "    task: forecasting\n",
      "  inference_node:\n",
      "    type: command\n",
      "    inputs:\n",
      "      test_data:\n",
      "        path: ${{parent.inputs.inference_data}}\n",
      "      model_path:\n",
      "        path: ${{parent.jobs.training_node.outputs.best_model}}\n",
      "      target_column_name: usage\n",
      "      forecast_mode: rolling\n",
      "      forecast_step: 24\n",
      "    outputs:\n",
      "      inference_output_file: ${{parent.outputs.forecast_output}}\n",
      "    resources:\n",
      "      instance_count: 1\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: automl_forecasting_inference\n",
      "      version: 0.0.1\n",
      "      display_name: AutoML Forecasting Inference\n",
      "      description: Inference component for AutoML Forecasting.\n",
      "      type: command\n",
      "      inputs:\n",
      "        test_data:\n",
      "          type: uri_folder\n",
      "          description: Test data folder with csv or parquet file.\n",
      "          optional: false\n",
      "        model_path:\n",
      "          type: mlflow_model\n",
      "          description: The trained AutoML Forecasting MLFLOW model.\n",
      "          optional: false\n",
      "        target_column_name:\n",
      "          type: string\n",
      "          optional: false\n",
      "          description: The name of the target column.\n",
      "        forecast_mode:\n",
      "          type: string\n",
      "          optional: false\n",
      "          default: recursive\n",
      "          description: The forecast mode used for inference, possible values are `recursive`\n",
      "            and `rolling`.\n",
      "          enum:\n",
      "          - recursive\n",
      "          - rolling\n",
      "        forecast_step:\n",
      "          type: integer\n",
      "          optional: true\n",
      "          default: '1'\n",
      "          description: 'The forecast step used for rolling forecast. See more details\n",
      "            here: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast?view=azureml-api-2#evaluating-model-accuracy-with-a-rolling-forecast'\n",
      "        forecast_quantiles:\n",
      "          type: string\n",
      "          optional: true\n",
      "          description: The quantiles for forecasting, comma separated string of float\n",
      "            values, e.g. '0.1,0.5,0.9'.\n",
      "      outputs:\n",
      "        inference_output_file:\n",
      "          type: uri_file\n",
      "          description: Inference output data file.\n",
      "        evaluation_config_output_file:\n",
      "          type: uri_file\n",
      "          description: The evaluation config file which can be consumed by the compute\n",
      "            metrics component.\n",
      "      command: python inference.py --test_data ${{inputs.test_data}} --model_path\n",
      "        ${{inputs.model_path}} --target_column_name ${{inputs.target_column_name}}\n",
      "        --forecast_mode ${{inputs.forecast_mode}} $[[--forecast_step '${{inputs.forecast_step}}']]\n",
      "        $[[--forecast_quantiles '${{inputs.forecast_quantiles}}']] --inference_output_file_name\n",
      "        ${{outputs.inference_output_file}} --evaluation_config_output_file_name ${{outputs.evaluation_config_output_file}}\n",
      "      environment: azureml://registries/azureml-staging/environments/automl-gpu/versions/0.1\n",
      "      code: azureml://registries/azureml-staging/codes/b35c71ed-59d7-4564-bb17-e2adf814be30/versions/1\n",
      "      resources:\n",
      "        instance_count: 1\n",
      "      id: azureml://registries/azureml-staging/components/automl_forecasting_inference/versions/0.0.1\n",
      "      is_deterministic: true\n",
      "      tags:\n",
      "        Preview: ''\n",
      "      creation_context:\n",
      "        created_at: '2023-07-06T18:31:02.652872+00:00'\n",
      "        created_by: Microsoft Staging\n",
      "        created_by_type: User\n",
      "        last_modified_at: '2023-07-06T18:31:02.652872+00:00'\n",
      "        last_modified_by: Microsoft Staging\n",
      "        last_modified_by_type: User\n",
      "  compute_metrics_node:\n",
      "    type: command\n",
      "    inputs:\n",
      "      ground_truth:\n",
      "        path: ${{parent.jobs.inference_node.outputs.inference_output_file}}\n",
      "      prediction:\n",
      "        path: ${{parent.jobs.inference_node.outputs.inference_output_file}}\n",
      "      evaluation_config:\n",
      "        path: ${{parent.jobs.inference_node.outputs.evaluation_config_output_file}}\n",
      "      task: tabular-forecasting\n",
      "    outputs:\n",
      "      evaluation_result: ${{parent.outputs.output_files}}\n",
      "    resources:\n",
      "      instance_count: 1\n",
      "    compute: azureml:demand-fcst-hts-cluster\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: compute_metrics\n",
      "      version: 0.0.18.preview\n",
      "      display_name: Compute Metrics\n",
      "      description: Calculate model performance metics, given ground truth and prediction\n",
      "        data.\n",
      "      type: command\n",
      "      inputs:\n",
      "        ground_truth:\n",
      "          type: uri_file\n",
      "          description: Ground Truths of Test Data as a 1-column JSON Lines file\n",
      "          optional: true\n",
      "        ground_truth_mltable:\n",
      "          type: mltable\n",
      "          description: Ground Truth of Test Data as MLTable folder\n",
      "          optional: true\n",
      "        prediction:\n",
      "          type: uri_file\n",
      "          description: Model Predictions as a 1-column JSON Lines file\n",
      "          optional: true\n",
      "        prediction_mltable:\n",
      "          type: mltable\n",
      "          description: Model Predictions as MLTable folder\n",
      "          optional: true\n",
      "        prediction_probabilities:\n",
      "          type: uri_file\n",
      "          description: Predictions Probabilities as 1-column JSON Lines file\n",
      "          optional: true\n",
      "        prediction_probabilities_mltable:\n",
      "          type: mltable\n",
      "          description: Predictions Probabilities as MLTable folder\n",
      "          optional: true\n",
      "        evaluation_config:\n",
      "          type: uri_file\n",
      "          description: Additional parameters required for evaluation.\n",
      "          optional: true\n",
      "        task:\n",
      "          type: string\n",
      "          optional: false\n",
      "          default: tabular-classification\n",
      "          description: Task type\n",
      "          enum:\n",
      "          - tabular-classification\n",
      "          - tabular-classification-multilabel\n",
      "          - tabular-regression\n",
      "          - tabular-forecasting\n",
      "          - text-classification\n",
      "          - text-classification-multilabel\n",
      "          - text-named-entity-recognition\n",
      "          - text-summarization\n",
      "          - question-answering\n",
      "          - text-translation\n",
      "          - text-generation\n",
      "          - fill-mask\n",
      "          - image-classification\n",
      "          - image-classification-multilabel\n",
      "        ground_truth_column_name:\n",
      "          type: string\n",
      "          optional: true\n",
      "          description: Column name which contains ground truths in provided uri file\n",
      "            for ground_truth. (Optional if we have one column name.)\n",
      "        prediction_column_name:\n",
      "          type: string\n",
      "          optional: true\n",
      "          description: Column name which contains ground truths in provided uri file\n",
      "            for prediction. (Optional if we have one column name.)\n",
      "        evaluation_config_params:\n",
      "          type: string\n",
      "          optional: true\n",
      "          description: JSON Serielized string of evaluation_config\n",
      "      outputs:\n",
      "        evaluation_result:\n",
      "          type: uri_folder\n",
      "      command: python compute_metrics.py --task ${{inputs.task}} $[[--ground_truths\n",
      "        '${{inputs.ground_truth}}']] $[[--ground_truths_mltable '${{inputs.ground_truth_mltable}}']]\n",
      "        $[[--predictions '${{inputs.prediction}}']] $[[--predictions_mltable '${{inputs.prediction_mltable}}']]\n",
      "        --output '${{outputs.evaluation_result}}' $[[--prediction_probabilities '${{inputs.prediction_probabilities}}']]\n",
      "        $[[--prediction_probabilities_mltable '${{inputs.prediction_probabilities_mltable}}']]\n",
      "        $[[--config-file-name '${{inputs.evaluation_config}}']] $[[--ground_truths_column_name\n",
      "        ${{inputs.ground_truth_column_name}}]] $[[--predictions_column_name ${{inputs.prediction_column_name}}]]\n",
      "        $[[--config_str '${{inputs.evaluation_config_params}}']]\n",
      "      environment: azureml://registries/azureml/environments/model-evaluation/versions/6\n",
      "      code: azureml://registries/ManyModels_HTS_BugBash/codes/166b561d-e674-48da-b5b8-53ce15c27abe/versions/1\n",
      "      resources:\n",
      "        instance_count: 1\n",
      "      id: azureml://registries/ManyModels_HTS_BugBash/components/compute_metrics/versions/0.0.18.preview\n",
      "      is_deterministic: true\n",
      "      tags:\n",
      "        type: evaluation\n",
      "        sub_type: compute_metrics\n",
      "        Preview: ''\n",
      "      creation_context:\n",
      "        created_at: '2023-05-30T23:18:19.478464+00:00'\n",
      "        created_by: Nikolay Rovinskiy\n",
      "        created_by_type: User\n",
      "        last_modified_at: '2023-05-30T23:18:19.478464+00:00'\n",
      "        last_modified_by: Nikolay Rovinskiy\n",
      "        last_modified_by_type: User\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_job = evaluation_pipeline(\n",
    "    training_data=Input(type=\"uri_folder\", path=\"./data/train\"),\n",
    "    inference_data=Input(type=\"uri_folder\", path=\"./data/test\"),\n",
    "    # validation_data=Input(type=\"uri_folder\", path=\"./data/valid\"),\n",
    ")\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = amlcompute_cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682997237524
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Field 'max_nodes': This is an experimental field, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: epic_boot_gwkkmlrfcj\n",
      "Web View: https://ml.azure.com/runs/epic_boot_gwkkmlrfcj?wsid=/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/vlbejan_eastus2_rg/workspaces/vlbejan_eastus2_new_ws\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-07-07 22:59:02Z] Submitting 1 runs, first five are: c2839fc6:45a63053-fb4f-4f5e-9207-648a5388ccd0\n",
      "[2023-07-07 23:11:25Z] Completing processing run id 45a63053-fb4f-4f5e-9207-648a5388ccd0.\n",
      "[2023-07-07 23:11:26Z] Submitting 1 runs, first five are: 37812287:3cb3f194-43ae-4ce0-b0bd-8eba464138ff\n",
      "[2023-07-07 23:15:09Z] Completing processing run id 3cb3f194-43ae-4ce0-b0bd-8eba464138ff.\n",
      "[2023-07-07 23:15:09Z] Submitting 1 runs, first five are: b4d79e1f:463b9aba-581a-475f-b390-8c98f76376fe\n",
      "[2023-07-07 23:16:39Z] Completing processing run id 463b9aba-581a-475f-b390-8c98f76376fe.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: epic_boot_gwkkmlrfcj\n",
      "Web View: https://ml.azure.com/runs/epic_boot_gwkkmlrfcj?wsid=/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/vlbejan_eastus2_rg/workspaces/vlbejan_eastus2_new_ws\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"single-model-experiment-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y%m%d\"\n",
    ")\n",
    "\n",
    "pipeline_submitted_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To rehydrate run\n",
    "# RUN_ID = \"epic_boot_gwkkmlrfcj\"\n",
    "# pipeline_submitted_job = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rehydrate run\n",
    "# RUN_ID = \"<Paste the PipelineRunId from the output of the previous cell.>\"\n",
    "# pipeline_submitted_job = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Kick Off the Baseline Experiment\n",
    "\n",
    "To establish a baseline, we will use the same pipeline as before with one minore change. We will add Naive model to the allowed model list and change the number of rolling origin cross validations (ROCV) to 2. Reducing the ROCV speeds up the runtime and is needed for model selection only, while in this run we have only one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_parameters.update(\n",
    "    {\n",
    "        \"allowed_training_algorithms\": [\"Naive\"],\n",
    "        \"n_cross_validations\": 2,\n",
    "        \"enable_dnn_training\": False,\n",
    "        \"training_mode\": None,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job_base = evaluation_pipeline(\n",
    "    training_data=Input(type=\"uri_folder\", path=\"./data/train\"),\n",
    "    inference_data=Input(type=\"uri_folder\", path=\"./data/test\"),\n",
    ")\n",
    "print(pipeline_job_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline level compute\n",
    "pipeline_job_base.settings.default_compute = amlcompute_cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_experiment_name = (\n",
    "    \"single-model-experiment-base-\" + datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    ")\n",
    "\n",
    "pipeline_submitted_job_base = ml_client.jobs.create_or_update(\n",
    "    pipeline_job_base,\n",
    "    experiment_name=base_experiment_name,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job_base.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rehydrate baseline run\n",
    "RUN_ID = \"lime_cushion_lljyk5r88m\"\n",
    "pipeline_submitted_job_base = ml_client.jobs.get(RUN_ID)\n",
    "pipeline_submitted_job_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rehydrate baseline run\n",
    "# RUN_ID = \"<Paste the PipelineRunId from the output of the previous cell.>\"\n",
    "# pipeline_submitted_job_base = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Pipeline Output\n",
    "Next, we will download the output files generated by the compute metrics components for each executed pipeline and save them in the corresponfing subfolder of the `output` folder. First, we create corresponding output directories. Then, we execute the `ml_client.jobs.download` command which downloads experiments' outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directories\n",
    "automl_output_dir = os.path.join(os.getcwd(), \"output/automl\")\n",
    "base_output_dir = os.path.join(os.getcwd(), \"output/base\")\n",
    "\n",
    "os.makedirs(automl_output_dir, exist_ok=True)\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job.name,\n",
    "    download_path=automl_output_dir,\n",
    "    output_name=\"output_files\",\n",
    ")\n",
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job_base.name,\n",
    "    download_path=base_output_dir,\n",
    "    output_name=\"output_files\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job.name,\n",
    "    download_path=automl_output_dir,\n",
    "    output_name=\"forecast_output\",\n",
    ")\n",
    "\n",
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job_base.name,\n",
    "    download_path=base_output_dir,\n",
    "    output_name=\"forecast_output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Compare Evaluation Results\n",
    "\n",
    "### 9.1. Examine Metrics\n",
    "\n",
    "In this section, we compare metrics for the 2 pipeline runs to quantify accuracy improvement of AutoML over the baseline model. First, we compare metrics that are calculated for the entire dataset. Since there are 10 unique time series in the test dataset, these individual metrics are aggregated into a single number. The non-normalized metrics can be misleading due to the difference in scales of each unique time series. The following [article (placeholder)](https://review.learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2&branch=pr-en-us-238443#forecasting-metrics-normalization-and-aggregation) explains this topic in a greater detail.\n",
    "\n",
    "The code in the next cell loads dataset metrics for each of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_artifacts_path = os.path.join(\n",
    "    \"named-outputs\", \"output_files\", \"evaluationResult\"\n",
    ")\n",
    "\n",
    "with open(os.path.join(automl_output_dir, metrics_artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_automl_series = json.load(f)\n",
    "\n",
    "with open(os.path.join(base_output_dir, metrics_artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_base_series = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge two dataframes to examine metrics side by side. The `metrics_all` data frame contains two columns which correspond to the scores from the many models and the baseline experiments, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_automl = (\n",
    "    pd.Series(metrics_automl_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "metrics_base = (\n",
    "    pd.Series(metrics_base_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "metrics_all = pd.DataFrame(\n",
    "    [metrics_automl_series, metrics_base_series], index=[\"score_automl\", \"score_base\"]\n",
    ").T\n",
    "metrics_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.1 Detailed Metrics\n",
    "\n",
    "Next, we will load and examine the detailed accuracy metrics since the aggregate metrics may not convey enough information to make a decision about product accuracy. It may be helpful to examine metrics at a more granular level. We will extract metrics per time series. To do this, we create a helper function `extract_specific_metric` which reads the JSON file and returns a specified metric for each time series. Even though the file contains the following metrics, we will  we will focus on the normalized root mean squared error (NRMSE) accuracy metric for illustration purposes. <ul>\n",
    "    <li> `explained_variance` </li>\n",
    "    <li> `mean_absolute_error` </li>\n",
    "    <li> `mean_absolute_percentage_error`</li>\n",
    "    <li> `median_absolute_error`</li>\n",
    "    <li> `normalized_median_absolute_error`</li>\n",
    "    <li> `normalized_root_mean_squared_error`</li>\n",
    "    <li> `normalized_root_mean_squared_error`</li>\n",
    "    <li> `normalized_root_mean_squared_log_error`</li>\n",
    "    <li> `r2_score`</li>\n",
    "    <li> `root_mean_squared_log_error`</li>\n",
    "    <li> `root_mean_squared_error`</li>\n",
    "    <li> `root_mean_squared_log_error`</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_specific_metric(path, metric_name):\n",
    "    with open(path) as f:\n",
    "        artifact = json.load(f)\n",
    "    all_metrics = pd.DataFrame(artifact[\"data\"])\n",
    "    index_scores = [\"customer_id\"] + [metric_name]\n",
    "    return all_metrics[index_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_table_relative_path = os.path.join(\n",
    "    metrics_artifacts_path, \"artifacts\", \"forecast_time_series_id_distribution_table\"\n",
    ")\n",
    "automl_metric = extract_specific_metric(\n",
    "    os.path.join(automl_output_dir, metrics_table_relative_path),\n",
    "    \"normalized_root_mean_squared_error\",\n",
    ")\n",
    "\n",
    "base_metric = extract_specific_metric(\n",
    "    os.path.join(base_output_dir, metrics_table_relative_path),\n",
    "    \"normalized_root_mean_squared_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = automl_metric.merge(\n",
    "    base_metric,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"inner\",\n",
    "    suffixes=[\"_automl\", \"_base\"],\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Generate Time Series Plots\n",
    "\n",
    "Here, we generate forecast versus actuals plot for the test set for both the best many models and the baseline. Since we use rolling evaluation with the step size of 24 hours, this mimics the behavior of putting both models in production and monitoring their behavior for the duration of the test set. This step helps you make informed decisions about model performance and saves numerous costs associated with productionalizing the model and monitoring its performance in real life. \n",
    "\n",
    "In the next block of code, we, load the test set output for each of the runs and merge the data. Then, we generate and save time series plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_table_relative_path = os.path.join(\n",
    "    \"named-outputs\", \"forecast_output\", \"inference_output_file\"\n",
    ")\n",
    "\n",
    "forecast_column_name = \"automl_predictions\"\n",
    "base_forecast_column_name = \"base_predictions\"\n",
    "actual_column_name = \"automl_actuals\"\n",
    "forecast_origin_column_name = \"automl_forecast_origin\"\n",
    "\n",
    "automl_fcst = pd.read_json(\n",
    "    os.path.join(automl_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "automl_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "base_fcst = pd.read_json(\n",
    "    os.path.join(base_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "base_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "print(automl_fcst.head(3), \"\\n---\")\n",
    "print(base_fcst.head(3), \"\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_table_relative_path = os.path.join(\n",
    "    \"named-outputs\", \"forecast_output\", \"inference_output_file\"\n",
    ")\n",
    "\n",
    "forecast_column_name = \"automl_prediction\"\n",
    "base_forecast_column_name = \"base_prediction\"\n",
    "actual_column_name = \"automl_actual\"\n",
    "forecast_origin_column_name = \"automl_forecast_origin\"\n",
    "\n",
    "automl_fcst = pd.read_json(\n",
    "    os.path.join(automl_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "automl_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "base_fcst = pd.read_json(\n",
    "    os.path.join(base_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "base_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "# automl_fcst = pd.read_parquet(os.path.join(automl_output_dir, forecast_table_relative_path))\n",
    "# base_fcst = pd.read_parquet(os.path.join(base_output_dir, forecast_table_relative_path))\n",
    "\n",
    "merge_columns = [\"customer_id\"] + [actual_column_name]\n",
    "merge_columns.extend([time_column_name, forecast_origin_column_name])\n",
    "\n",
    "backtest = automl_fcst.merge(\n",
    "    base_fcst.rename(columns={forecast_column_name: base_forecast_column_name}),\n",
    "    on=merge_columns,\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(f\"AutoML forecast table size: {automl_fcst.shape}\\n---\")\n",
    "print(f\"Base forecast table size: {base_fcst.shape}\\n---\")\n",
    "print(f\"Merged forecast table size: {backtest.shape}\\n---\")\n",
    "backtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper_scripts import draw_one_plot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plot_filename = \"forecast_vs_actual.pdf\"\n",
    "\n",
    "pdf = PdfPages(os.path.join(os.getcwd(), \"./output\", plot_filename))\n",
    "for _, one_forecast in backtest.groupby(\"customer_id\"):\n",
    "    one_forecast[time_column_name] = pd.to_datetime(one_forecast[time_column_name])\n",
    "    one_forecast.sort_values(time_column_name, inplace=True)\n",
    "    draw_one_plot(\n",
    "        one_forecast,\n",
    "        time_column_name,\n",
    "        target_column_name,\n",
    "        [\"customer_id\"],\n",
    "        [actual_column_name, forecast_column_name, base_forecast_column_name],\n",
    "        pdf,\n",
    "        plot_predictions=True,\n",
    "    )\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(os.path.join(\"./output/forecast_vs_actual.pdf\"), width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deployment\n",
    "\n",
    "In this section, we will illustrate how to deploy and inference models using batch endpoint. Batch endpoints are endpoints that are used to do batch inferencing on large volumes of data in asynchronous way. Batch endpoints receive pointers to data and run jobs asynchronously to process the data in parallel on compute clusters and store outputs to a datastore for further analysis. For more information on batch endpoints see this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-batch?view=azureml-api-2).\n",
    "\n",
    "### 10.1. Create Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete once components are in the public regitry\n",
    "from azure.ai.ml.constants._common import AZUREML_PRIVATE_FEATURES_ENV_VAR\n",
    "\n",
    "os.environ[AZUREML_PRIVATE_FEATURES_ENV_VAR] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: sdk-single-model-zfuvp\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Creating a unique endpoint name by including a random suffix\n",
    "allowed_chars = string.ascii_lowercase + string.digits\n",
    "endpoint_suffix = \"\".join(random.choice(allowed_chars) for x in range(5))\n",
    "endpoint_name = \"sdk-single-model-\" + endpoint_suffix\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = BatchEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"An endpoint for component deployments\",\n",
    "    properties={\"ComponentDeployment.Enabled\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the Endpoint in the workspace usign the MLClient created earlier. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.ai.ml._restclient.v2022_05_01.models._models_py3.BatchEndpointData at 0x7f6b6271bca0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Create the Deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class PipelineComponentBatchDeployment: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "deployment = PipelineComponentBatchDeployment(\n",
    "    name=\"sdk-single-model-deployment\",\n",
    "    description=\"A single deployment.\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    component=inference_component.id,\n",
    "    settings={\"default_compute\": amlcompute_cluster_name},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the deployment in the workspace usign the MLClient created earlier. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "Operation returned an invalid status 'OK'\nContent: {\n  \"id\": \"/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/providers/Microsoft.MachineLearningServices/locations/eastus2/mfeOperationsStatus/bdbes:68b1ffd2-f656-4473-90c7-b92e09efab20:1fe4fb43-f22d-411c-828d-ecb9af3c3137\",\n  \"name\": \"bdbes:68b1ffd2-f656-4473-90c7-b92e09efab20:1fe4fb43-f22d-411c-828d-ecb9af3c3137\",\n  \"status\": \"Failed\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationFailed\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/base_polling.py:515\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadStatus \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/base_polling.py:553\u001b[0m, in \u001b[0;36mLROBasePolling._poll\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _failed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus()):\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationFailed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation failed or canceled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    555\u001b[0m final_get_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39mget_final_get_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response)\n",
      "\u001b[0;31mOperationFailed\u001b[0m: Operation failed or canceled",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_deployments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:256\u001b[0m, in \u001b[0;36mLROPoller.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# type: (Optional[int]) -> PollingReturnType\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the result of the long running operation, or\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    the result available after the specified timeout.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    :raises ~azure.core.exceptions.HttpResponseError: Server problem with the query.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_polling_method\u001b[38;5;241m.\u001b[39mresource()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:276\u001b[0m, in \u001b[0;36mLROPoller.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread\u001b[38;5;241m.\u001b[39mjoin(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# Let's handle possible None in forgiveness here\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/8165\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# Was None\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:192\u001b[0m, in \u001b[0;36mLROPoller._start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"Start the long running operation.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03mOn completion, runs any callbacks.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m:param callable update_cmd: The API request to check the status of\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m the operation.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_polling_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error\u001b[38;5;241m.\u001b[39mcontinuation_token:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/base_polling.py:532\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(\n\u001b[1;32m    526\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response,\n\u001b[1;32m    527\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(err),\n\u001b[1;32m    528\u001b[0m         error\u001b[38;5;241m=\u001b[39merr,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailed \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(\n\u001b[1;32m    533\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response, error\u001b[38;5;241m=\u001b[39merr\n\u001b[1;32m    534\u001b[0m     )\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: Operation returned an invalid status 'OK'\nContent: {\n  \"id\": \"/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/providers/Microsoft.MachineLearningServices/locations/eastus2/mfeOperationsStatus/bdbes:68b1ffd2-f656-4473-90c7-b92e09efab20:1fe4fb43-f22d-411c-828d-ecb9af3c3137\",\n  \"name\": \"bdbes:68b1ffd2-f656-4473-90c7-b92e09efab20:1fe4fb43-f22d-411c-828d-ecb9af3c3137\",\n  \"status\": \"Failed\"\n}"
     ]
    }
   ],
   "source": [
    "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Invoke the Endpoint\n",
    "\n",
    "The next cell contians the command that invokes the endpoint for batch inference job. The `invoke` method contains the `inputs` parameter. This parameter contains the inputs necessary to execute the inference component on the endpoint. To convince yourself this is the case, compare the input parameters for the `inference_component_from_registry` in section 6.3 with the `inputs` we are proving in the next cell. They are identical.\n",
    "\n",
    "Notice, the the `forecast_mode` is set to `\"recursive\"`. In the evaluation pipeline this component was used to generate rolling forecast to evalaute model performance on the test set. For more details on rolling evaluation, see our [forecasting model evaluation article](placeholder). Here, we are using it to generate a forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"single-model-experiment-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y%m%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(inference_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=endpoint.name,\n",
    "    deployment_name=deployment.name,\n",
    "    inputs={\n",
    "        \"training_data\": Input(type=AssetTypes.URI_FOLDER, path=\"./data/inference\"),\n",
    "        \"train_experiment_name\": Input(type=\"string\", default=experiment_name),\n",
    "        \"max_nodes\": Input(type=\"integer\", default=1),\n",
    "        \"max_concurrency_per_node\": Input(type=\"integer\", default=2),\n",
    "        \"compute_name\": Input(type=\"string\", default=amlcompute_cluster_name),\n",
    "        \"forecast_mode\": Input(type=\"string\", default=\"recursive\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will stream the job output to monitor the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = job.name\n",
    "batch_job = ml_client.jobs.get(name=job_name)\n",
    "print(f\"Batch job status: {batch_job.status}\\n---\")\n",
    "ml_client.jobs.stream(name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4. Download Forecast Output\n",
    "\n",
    "Finally, we download the forecast output and print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_output_dir = os.path.join(os.getcwd(), \"forecast\")\n",
    "\n",
    "for child in ml_client.jobs.list(parent_job_name=job.name):\n",
    "    print(f\"{child.name}\\n---\")\n",
    "    if (\n",
    "        child.properties[\"azureml.moduleName\"]\n",
    "        == \"automl_many_model_inferencing_collect\"\n",
    "    ):\n",
    "        print(\"Downloading data ...\\n---\")\n",
    "        ml_client.jobs.download(\n",
    "            child.name, download_path=fcst_output_dir, output_name=\"metadata\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_df = pd.read_parquet(\n",
    "    os.path.join(fcst_output_dir, \"named-outputs\", \"metadata\", \"raw_forecast\")\n",
    ")\n",
    "fcst_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5. [Optional] Delete the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint.name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
