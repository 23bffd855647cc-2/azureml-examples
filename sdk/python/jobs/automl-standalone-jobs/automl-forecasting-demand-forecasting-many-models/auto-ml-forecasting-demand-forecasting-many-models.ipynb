{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "**Demand Forecasting Using Many Models**\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Compute](#Compute)\n",
    "1. [Data](#Data)\n",
    "1. [Import Components From Registry](#ImportComponents)\n",
    "1. [Create a Pipeline](#Pipeline)\n",
    "1. [Kick Off Pipeline Runs](#PipelineRuns)\n",
    "1. [Download Output](#DownloadOutput)\n",
    "1. [Compare Evaluation Results](#CompareResults)\n",
    "1. [Deployment](#Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The objective of this notebook is to illustrate how to use the component-based AutoML many models solution accelerator for demand forecasting tasks. It walks you through all stages of model evaluation and production process starting with data ingestion and concluding with batch endpoint deployment for production.\n",
    "\n",
    "We use a subset of UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)) with the objective of predicting electricity demand per consumer 24 hours ahead. The data was preprocessed using the [data prep notebook](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-data-preparation/auto-ml-forecasting-data-preparation.ipynb). Please refer to it for illustration on how to download the data from the source, aggregate to an hourly frequency, convert from wide to long format and upload to the Datastore. Here, we will work with the already uploaded data. \n",
    "\n",
    "Having a problem description such as to generate accurate forecasts 24 hours ahead sounds like a relatively straight forward task. However, there are quite a few steps a user needs to take before the model is put in production. A user needs to prepare the data, partition it into appropriate sets, select the best model, evaluate it against a baseline, and monitor the model in real life to collect enough observations on how it would perform had it been put in production. Some of these steps are time consuming, some require certain expertise in writing code. The steps shown in this notebook follow a typical thought process one follows before the model is put in production.\n",
    "\n",
    "Make sure you have executed the [configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1682992408484
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import (\n",
    "    Environment,\n",
    "    BuildContext,\n",
    "    Model,\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    CodeConfiguration,\n",
    "    BatchEndpoint,\n",
    "    BatchDeployment,\n",
    "    AmlCompute,\n",
    ")\n",
    "from azure.ai.ml.entities._deployment.job_definition import JobDefinition\n",
    "\n",
    "# print the sdk version - you many want to share this in the issue you will report if parts of this notebook don't work\n",
    "!pip show azure-ai-ml\n",
    "os.environ[\"AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential does not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "    print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.connections._subscription_id\n",
    "output[\"Resource Group\"] = workspace.resource_group\n",
    "output[\"Location\"] = workspace.location\n",
    "pd.DataFrame(data=output, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute \n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "\n",
    "You will need to create a compute target for your AutoML run. In this tutorial, you will create AmlCompute as your training compute resource.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "\n",
    "Here, we use a 5 node cluster of the `STANDARD_DS15_V2` series for illustration purposes. You will need to adjust the compute type and the number of nodes based on your needs which can be driven by the speed needed for model seelction, data size, etc. \n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "amlcompute_cluster_name = \"demand-fcst-mm-cluster\"\n",
    "\n",
    "try:\n",
    "    # Retrieve an already attached Azure Machine Learning Compute.\n",
    "    compute_target = ml_client.compute.get(amlcompute_cluster_name)\n",
    "except ResourceNotFoundError as e:\n",
    "    compute_target = AmlCompute(\n",
    "        name=amlcompute_cluster_name,\n",
    "        size=\"STANDARD_DS15_V2\",\n",
    "        type=\"amlcompute\",\n",
    "        min_instances=0,\n",
    "        max_instances=5,\n",
    "        idle_time_before_scale_down=600,\n",
    "    )\n",
    "    poller = ml_client.begin_create_or_update(compute)\n",
    "    poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data\n",
    "\n",
    "For illustration purposes we use the UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)). The original dataset contians electricity consumption data for 370 consumers measured at 15 minute intervals. We aggregated the data to an hourly frequency and convereted to the kilowatt hours (kWh) for 10 customers. The following cells read and prints the first few rows of the training data as well as print the number of uique time series in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = \"datetime\"\n",
    "target_column_name = \"usage\"\n",
    "time_series_id_column_names = [\"customer_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"train\"\n",
    "df = pd.read_parquet(\n",
    "    f\"./data/{dataset_type}/uci_electro_small_mm_{dataset_type}.parquet\"\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nseries = df.groupby(time_series_id_column_names).ngroups\n",
    "print(f\"Data contains {nseries} individual time-series\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import Components From Registry\n",
    "\n",
    "An Azure Machine Learning component is a self-contained piece of code that does one step in a machine learning pipeline. A component is analogous to a function - it has a name, inputs, outputs, and a body. Components are the building blocks of the Azure Machine Learning pipelines. It's a good engineering practice to build a machine learning pipeline to split a complete machine learning task into a multi-step workflow. Such that, everyone can work on the specific step independently. In Azure Machine Learning, a component represents one reusable step in a pipeline. Components are designed to help improve the productivity of pipeline building. Specifically, components offer:\n",
    "- Well-defined interface: Components require a well-defined interface (input and output). The interface allows the user to build steps and connect steps easily. The interface also hides the complex logic of a step and removes the burden of understanding how the step is implemented.\n",
    "\n",
    "- Share and reuse: As the building blocks of a pipeline, components can be easily shared and reused across pipelines, workspaces, and subscriptions. Components built by one team can be discovered and used by another team.\n",
    "\n",
    "- Version control: Components are versioned. The component producers can keep improving components and publish new versions. Consumers can use specific component versions in their pipelines. This gives them compatibility and reproducibility.\n",
    "\n",
    "For a more detailed information on this subject, refer to the this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2).\n",
    "\n",
    "To import components,  we need to get the registry. The following command obtains the public regsitry from which we will import components for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682992409837
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get registry\n",
    "# TODO: Change registry name once the components are in the public registry\n",
    "ml_client_registry = MLClient(\n",
    "    credential=credential, registry_name=\"ManyModels_HTS_BugBash\"\n",
    ")\n",
    "print(ml_client_registry)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: delete this cell once the components are in the public registry\n",
    "preview_registry = \"ForecastingDemand2\"\n",
    "preview_registry_ml_client = MLClient(\n",
    "    credential,\n",
    "    ml_client.subscription_id,\n",
    "    \"nirovins-southcentralus-rg\",\n",
    "    registry_name=preview_registry,\n",
    ")\n",
    "print(preview_registry_ml_client)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pull specific components and use them to build a pipeline of steps. For the illustration of the product evaluation workflow we will use the following components:\n",
    "- Data partitioning component: allows users to partion the data for many models runs, both, training and inference.\n",
    "- Many models training component: trains the best model per partition specified by users.\n",
    "- Many moodels inference componnet: generates forecast for each partition. This can be done on the test and inference sets.\n",
    "- Compute metrics component: calculates metrics per time series if inference component was used on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_component_from_registry = ml_client_registry.components.get(\n",
    "    name=\"automl_solution_accelerators_partition\", label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Data partitioning component version: {partition_component_from_registry.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682992443218
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_component_from_registry = ml_client_registry.components.get(\n",
    "    name=\"automl_many_model_training\",\n",
    "    label=\"latest\",\n",
    ")\n",
    "print(\n",
    "    f\"Many models training component version: {train_component_from_registry.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996405805
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_component_from_registry = ml_client_registry.components.get(\n",
    "    name=\"automl_many_model_inferencing\", label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Many models inference component version: {train_component_from_registry.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996406713
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "compute_metrics_component = preview_registry_ml_client.components.get(\n",
    "    name=\"compute_metrics\", label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Many models inference component version: {compute_metrics_component.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1682996406974
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## 6. Create a Pipeline\n",
    "\n",
    "Now, that we imported the components we will build an evaluation pipeline. This pipeline will allow us to partition the data, train best models for each partition, genererate rolling forecasts on the test set, and, finally, calculate metrics on the test set output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Create a JSON\n",
    "\n",
    "AzureML components can only receive specific object types such as strings, JSON files, URI Folders and URI Files. Other object types are not accepted. As a result, we need to create a JSON file. It will be passed into the training component, which, in turn, will convert this information to the `AutoMLConfig` object.\n",
    "\n",
    "The following are the bare-minimum parameters needed to successfully train many models. For a finer control of the experiment a user may add other parameters to the config file. See [AutoMLConfig documentation](https://learn.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py) for a complete list of available parameters. \n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **task**                           | forecasting |\n",
    "| **primary_metric**                 | This is the metric that you want to optimize. Forecasting supports the following primary metrics<ul><li>`normalized_root_mean_squared_error`</li><li>`normalized_mean_absolute_error`</li><li>`spearman_correlation`</li><li>`r2_score`</li></ul> We recommend using either the normalized root mean squared error or normalized mean absolute erorr as a primary metric because they measure forecast accuracy. See the [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-automl-forecasting-faq#how-do-i-choose-the-primary-metric) for a more detailed discussion on this topic. |\n",
    "| **label_column_name**      | The name of the target column we are trying to predict. |\n",
    "| **time_column_name**       | The name of your time column. |\n",
    "| **time_series_id_column_names** | The column names used to uniquely identify timeseries in data that has multiple rows with the same timestamp. |\n",
    "| **enable_early_stopping**  | Flag to enable early termination if the primary metric is no longer improving. |\n",
    "| **partition_column_names** | The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series. |\n",
    "| **allow_multi_partitions** | A flag that allows users to train one model per partition when each partition contians more than one unique time series. The dafault value is `Fasle`. |\n",
    "| **track_child_runs**       | Flag to disable tracking of child runs. Only best run is tracked if the flag is set to False (this includes the model and metrics of the run). |\n",
    "| **enable_early_stopping**  | Flag to enable early termination if the primary metric is no longer improving. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_horizon = 24\n",
    "target_column_name = \"usage\"\n",
    "time_column_name = \"date\"\n",
    "time_series_id_column_names = [\"customer_id\"]\n",
    "partition_column_names = [\"customer_id\"]\n",
    "allow_multi_partitions = False\n",
    "retrain_failed_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Required parameters\n",
    "automl_settings = dict(\n",
    "    task=\"forecasting\",\n",
    "    primary_metric=\"normalized_root_mean_squared_error\",\n",
    "    debug_log=\"debug.txt\",\n",
    "    label_column_name=target_column_name,\n",
    "    time_column_name=time_column_name,\n",
    "    forecast_horizon=max_horizon,\n",
    "    time_series_id_column_names=time_series_id_column_names,\n",
    "    partition_column_names=partition_column_names,\n",
    "    track_child_runs=False,\n",
    "    enable_early_stopping=True,\n",
    "    allow_multi_partitions=allow_multi_partitions,\n",
    ")\n",
    "pd.DataFrame(data=automl_settings, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save these settings as a JSON object. This object will be the input for the many models training component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_object = json.dumps(automl_settings)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"./automl_settings.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Provide additional pipeline parameters\n",
    "\n",
    "The next set of parameters is necessary to build the pipeline of components. These parameters are specific to the many models training and/or inference components. Since both training and inference component rely on the Parallel run step (PRS) to train/inference multiple models at once, you will need to determine the appropriate number of workers and nodes for your use case. The `max_concurrency_per_instance` is based off the number of cores of the compute VM. The `instance_count` will determine the number of master nodes to use, increasing the node count will speed up the training process.\n",
    "\n",
    "|Property|Description|\n",
    "|:-|:-|\n",
    "| **instance_count**                     | The number of compute nodes in a cluster to be used for training and inferencing steps. We recommend to start with 3 and increase the node_count if the training time is taking too long. |\n",
    "| **max_concurrency_per_instance**   | Process count per node. We recommend a 2:1 ratio for number of cores to the number of processes per node. For example, if a node has 16 cores then configure 8 **or less** process counts per node for optimal performance. |\n",
    "|**retrain_failed_model**| If training a model for any partition fails, should AutoML kick off a new child run for that partition? Possible values are `True` or `False`.|\n",
    "|**forecast_mode**| Type of forecat to perform on the test set. Can be `recursive` or `rolling`. Rolling forecast can be used for the evaluation purposes |\n",
    "| **prs_timeout_seconds**         | Maximum amount of time in seconds that the `ParallelRunStep` class is allowed. This is optional but provides customers with greater control on exit criteria. This must be greater than `experiment_timeout_hours` by at least 300 seconds. |\n",
    "|**partition_column_names**| The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series. This parameter is identical to the one in the `automl_config` object.|\n",
    "|**compute_name**| Name of the compute to execute the pipeline on. |\n",
    "|**enable_event_logger**| Set this value to `True` to enable event logger. |\n",
    "| **input_type**               | Type of file format for the input data. Supported options are `csv` and `parquet`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline parameters\n",
    "pipeline_parameters = dict(\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=5,\n",
    "    retrain_failed_model=True,\n",
    "    forecast_mode=\"rolling\",\n",
    "    prs_timeout_seconds=3700,\n",
    "    partition_column_names=partition_column_names,\n",
    "    compute_name=amlcompute_cluster_name,\n",
    "    enable_event_logger=True,\n",
    "    input_type=\"csv\",\n",
    ")\n",
    "print(\"Pipeline parameters\\n---\")\n",
    "pd.DataFrame(data=pipeline_parameters, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data partioning component allows us to partition the data in several ways. For \"small\" datasets the data can be loaded into the memory of a single node of the compute you plan to run the experiments on. If the data is too large to be partitioned on a single node, we need to use spark cluster for this step. Even though the data we are working with is small, we modified this notebook to handle both sets of scenarios. If you choose to run a spark job, we need to specify a separate set of parameters to the pipeline builder which must incude the following:\n",
    "\n",
    "|Property|Description|\n",
    "|:-|:-|\n",
    "| **instance_type**            | A key that defines the compute instance type to be used for the serverless Spark compute. The following instance types are currently supported:<ul><li>`Standard_E4S_V3`</li><li>`Standard_E8S_V3`</li><li>`Standard_E16S_V3`</li><li>`Standard_E32S_V3`</li><li>`Standard_E64S_V3`</li></ul>|\n",
    "| **runtime_version**          | A key that defines the Spark runtime version. The following Spark runtime versions are currently supported:<ul><li>`3.1.0`</li><li>`3.2.0`</li></ul> |\n",
    "| **driver_cores**       | The he number of cores allocated for the Spark driver. |\n",
    "| **driver_memory**      | The allocated memory for the Spark exedriver, with a size unit suffix `k`, `m`, `g` or `t` (for example, `512m`, `2g`). |\n",
    "| **executor_cores**     | The number of cores allocated for the Spark executor. |\n",
    "| **executor_memory**    | The allocated memory for the Spark executor, with a size unit suffix `k`, `m`, `g` or `t` (for example, `512m`, `2g`). |\n",
    "| **executor_instances** | The number of Spark executor instances|\n",
    "\n",
    "All of these parameters are described in [this document](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-submit-spark-jobs?view=azureml-api-2&tabs=sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SPARK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark parameters (optional)\n",
    "if USE_SPARK:\n",
    "    spark_parameters = dict(\n",
    "        instance_type=2,\n",
    "        runtime_version=\"3.2.0\",\n",
    "        driver_cores=1,\n",
    "        driver_memory=\"2g\",\n",
    "        executor_cores=2,\n",
    "        executor_memory=\"2g\",\n",
    "        executor_instances=2,\n",
    "    )\n",
    "    print(\"Spark parameters\\n---\")\n",
    "    pd.DataFrame(data=pipeline_parameters, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Build a Pipeline\n",
    "\n",
    "\n",
    "Next, we build the pipeline from the components. Since this notebook is designed to illustrate the evaluation flow, we will string these componenets in the folloiwng fashion. First, we partition the training data. Next, we train the best model for each partition. Then, we generaring a rolling forecast with the step size of 24 (hours) on the test set. Finally, we compute metrics based on the rolling forecast output from the revious step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996407118
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def mm_train_inference_components(\n",
    "    raw_data,\n",
    "    inference_data,\n",
    "    automl_config,\n",
    "    pipeline_parameters,\n",
    "    spark_parameters={},\n",
    "    use_spark=USE_SPARK,\n",
    "):\n",
    "    # 0. Extract pipeline parameters from the dictionary\n",
    "    partition_column_names = pipeline_parameters.get(\"pipeline_parameters\")\n",
    "    compute_name = pipeline_parameters.get(\"compute_name\")\n",
    "    max_concurrency_per_instance = pipeline_parameters.get(\n",
    "        \"max_concurrency_per_instance\"\n",
    "    )\n",
    "    prs_step_timeout = pipeline_parameters.get(\"prs_step_timeout\", 3700)\n",
    "    instance_count = pipeline_parameters.get(\"instance_count\", 1)\n",
    "    enable_event_logger = pipeline_parameters.get(\"enable_event_logger\", True)\n",
    "    retrain_failed_model = pipeline_parameters.get(\"retrain_failed_model\", True)\n",
    "    forecast_mode = pipeline_parameters.get(\"forecast_mode\", \"recursive\")\n",
    "    input_type = pipeline_parameters.get(\"input_type\", \"csv\")\n",
    "\n",
    "    # 1. Data partitioning step\n",
    "    partition_step = partition_component_from_registry(\n",
    "        raw_data=raw_data,\n",
    "        partition_column_names=partition_column_names,\n",
    "        input_type=pipeline_parameters.get(\"input_type\", \"csv\"),\n",
    "    )\n",
    "\n",
    "    if use_spark:\n",
    "        partition_step.resources = {\n",
    "            \"instance_type\": spark_parameters.get(\"instance_type\", \"Standard_E4S_V3\"),\n",
    "            \"runtime_version\": str(spark_parameters.get(\"runtime_version\", \"3.2.0\")),\n",
    "        }\n",
    "        partition_step.conf = {\n",
    "            \"spark.driver.cores\": spark_parameters.get(\"driver_cores\", 1),\n",
    "            \"spark.driver.memory\": str(spark_parameters.get(\"driver_memory\", \"2g\")),\n",
    "            \"spark.executor.cores\": spark_parameters.get(\"executor_cores\", 2),\n",
    "            \"spark.executor.memory\": str(spark_parameters.get(\"executor_memory\", \"2g\")),\n",
    "            \"spark.executor.instances\": spark_parameters.get(\"executor_instances\", 2),\n",
    "        }\n",
    "        partition_step.outputs.partitioned_data.mode = \"direct\"\n",
    "\n",
    "    # 2. Model training step\n",
    "    mm_train = train_component_from_registry(\n",
    "        raw_data=partition_step.outputs.partitioned_data,\n",
    "        automl_config=automl_config,\n",
    "        max_concurrency_per_instance=max_concurrency_per_instance,\n",
    "        prs_step_timeout=prs_step_timeout,\n",
    "        instance_count=instance_count,\n",
    "        enable_event_logger=enable_event_logger,\n",
    "        retrain_failed_model=retrain_failed_model,\n",
    "        compute_name=compute_name,\n",
    "    )\n",
    "\n",
    "    # 3. Inferencing step\n",
    "    mm_inference = inference_component_from_registry(\n",
    "        raw_data=inference_data,\n",
    "        enable_event_logger=enable_event_logger,\n",
    "        instance_count=instance_count,\n",
    "        max_concurrency_per_instance=max_concurrency_per_instance,\n",
    "        prs_step_timeout=prs_step_timeout,\n",
    "        optional_train_metadata=mm_train.outputs.run_output,\n",
    "        forecast_mode=forecast_mode,\n",
    "        compute_name=compute_name,\n",
    "    )\n",
    "\n",
    "    # 4. Metrics calculation step\n",
    "    compute_metrics_node = compute_metrics_component(\n",
    "        task=\"tabular-forecasting\",\n",
    "        prediction=mm_inference.outputs.evaluation_data,\n",
    "        ground_truth=mm_inference.outputs.evaluation_data,\n",
    "        evaluation_config=mm_inference.outputs.evaluation_configs,\n",
    "    )\n",
    "    compute_metrics_node.compute = compute_name\n",
    "\n",
    "    # 5. Specify pipeline outputs\n",
    "    return {\"output_files\": compute_metrics_node.outputs.evaluation_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Kick Off Pipeline Runs\n",
    "\n",
    "Now that the pipline is defined, we will use it to kick off several run. First, we will kick off an experiment which will evaluate the performance for the best AutoML model for each partition. Next, we will kick a pipeline which will only use the naive model for the same partitions. This will allow us to establish a baseline and compare performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Kick Off Best Many Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996407285
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = mm_train_inference_components(\n",
    "    raw_data=Input(type=\"uri_folder\", path=\"./data/train_small\"),\n",
    "    inference_data=Input(type=\"uri_folder\", path=\"./data/test_small\"),\n",
    "    automl_config=Input(type=\"uri_file\", path=\"./automl_settings_mm.json\"),\n",
    "    pipeline_parameters=pipeline_parameters,\n",
    ")\n",
    "if not USE_SPARK:\n",
    "    pipeline_job.settings.default_compute = amlcompute_cluster_name\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682997237524
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "experiment_name = \"mm-experiment-\" + datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "pipeline_submitted_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job,\n",
    "    experiment_name=experiment_name,\n",
    "    skip_validation=True,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To rehydrate run\n",
    "# RUN_ID = <Paste the PipelineRunId from the output of the previous cell.>\n",
    "# pipeline_submitted_job = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Kick Off the Baseline Experiment\n",
    "\n",
    "To establish a baseline, we will use the same pipeline as before with one minore change. We will add Naive model to the allowed model list and change the number of rolling origin cross validations (ROCV) to 2. Reducting the ROCV speeds up te runtime and has no effect on the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_settings = automl_settings.copy()\n",
    "baseline_settings.update({\"allowed_models\": \"Naive\", \"n_cross_validations\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we have done in section 6.3, we save the baseline experiment settings to as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(baseline_settings)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"./automl_settings_base.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job_base = mm_train_inference_components(\n",
    "    raw_data=Input(type=\"uri_folder\", path=\"./data/train\"),\n",
    "    inference_data=Input(type=\"uri_folder\", path=\"./data/test\"),\n",
    "    automl_config=Input(type=\"uri_file\", path=\"./automl_settings_base.json\"),\n",
    "    pipeline_parameters=pipeline_parameters,\n",
    ")\n",
    "pipeline_job_base.settings.default_compute = \"mm-compute\"\n",
    "print(pipeline_job_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_submitted_job_base = ml_client.jobs.create_or_update(\n",
    "    pipeline_job_base,\n",
    "    experiment_name=experiment_name,\n",
    "    skip_validation=True,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job_base.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To rehydrate baseline run\n",
    "# RUN_ID = <Paste the PipelineRunId from the output of the previous cell.>\n",
    "# pipeline_submitted_job_base = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Pipeline Output\n",
    "Next, we will download the output files generated by the compute metrics components for each experiment and save them in the corresponfing subfolder of the `output` folder. First, we create corresponding output directories. Then, we execute the `ml_client.jobs.download` command which downloads experiment outputs into the corresponding folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directories\n",
    "mm_output_dir = os.path.join(os.getcwd(), \"output/many-models\")\n",
    "base_output_dir = os.path.join(os.getcwd(), \"output/base\")\n",
    "\n",
    "os.makedirs(mm_output_dir, exist_ok=True)\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    pipeline_submitted_job.name, download_path=mm_output_dir, output_name=\"output_files\"\n",
    ")\n",
    "ml_client.jobs.download(\n",
    "    pipeline_submitted_job_base.name,\n",
    "    download_path=base_output_dir,\n",
    "    output_name=\"output_files\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Compare Evaluation Results\n",
    "\n",
    "### 9.1. Examine Metrics\n",
    "\n",
    "In this section, we compare metrics for the 2 pipeline runs to quantify accuracy improvement of AutoML over the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_path = os.path.join(\"named-outputs\", \"output_files\", \"evaluationResult\")\n",
    "\n",
    "with open(os.path.join(mm_output_dir, artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_hts_series = json.load(f)\n",
    "\n",
    "with open(os.path.join(base_output_dir, artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_base_series = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_hts = (\n",
    "    pd.Series(metrics_hts_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "metrics_base = (\n",
    "    pd.Series(metrics_base_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "\n",
    "metrics_all = metrics_hts.merge(\n",
    "    metrics_base, on=\"index\", how=\"inner\", suffixes=[\"_mm\", \"_base\"]\n",
    ")\n",
    "metrics_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Plot Metrics\n",
    "\n",
    "- Need Nikolay's changes to be reflected in the component and the environment it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Time Series Plots\n",
    "\n",
    "- Need Nikolay's changes to be reflected in the component and the environment it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deployment\n",
    "\n",
    "In this section, we will illustrate how to deploy and inference models using batch endpoint. Batch endpoints are endpoints that are used to do batch inferencing on large volumes of data over in asynchronous way. Batch endpoints receive pointers to data and run jobs asynchronously to process the data in parallel on compute clusters and store outputs to a data store for further analysis. For more information on batch endpoints see this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-batch?view=azureml-api-2).\n",
    "\n",
    "### 10.1. Create Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete once components are in the public regitry\n",
    "import os\n",
    "from azure.ai.ml.constants._common import AZUREML_PRIVATE_FEATURES_ENV_VAR\n",
    "\n",
    "os.environ[AZUREML_PRIVATE_FEATURES_ENV_VAR] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Creating a unique endpoint name by including a random suffix\n",
    "allowed_chars = string.ascii_lowercase + string.digits\n",
    "endpoint_suffix = \"\".join(random.choice(allowed_chars) for x in range(5))\n",
    "endpoint_name = \"sdk-many-models-\" + endpoint_suffix\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = BatchEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"A many models endpoint for component deployments\",\n",
    "    properties={\"ComponentDeployment.Enabled\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the Endpoint in the workspace usign the MLClient created earlier. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Create the Deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. You do not have to change anything in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = BatchDeployment(\n",
    "    name=\"sdk-many-models-deployment\",\n",
    "    description=\"A many models deployment.\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    compute=amlcompute_cluster_name,\n",
    "    job_definition=JobDefinition(\n",
    "        type=\"pipeline\",\n",
    "        component=inference_component_from_registry.id,\n",
    "        settings={\"compute\": amlcompute_cluster_name},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the deployment in the workspace usign the MLClient created earlier. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we update the default deployment name in the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.batch_endpoints.get(endpoint_name)\n",
    "endpoint.defaults.deployment_name = deployment.name\n",
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Invoke the Endpoint\n",
    "\n",
    "The next cell contians the command that invokes the endpoint for batch inference job. The `invoke` method contains the `inputs` parameter. This parameter contains the inputs necessary to execute the inference component on the endpoint. To convince yourself this is the case, compare the input parameters for the `inference_component_from_registry` in section 6.3 with the `inputs` we are proving in the next cell. They are identical.\n",
    "\n",
    "Notice, the the `forecast_mode` is set to `\"recursive\"`. In the evaluation pipeline this component was used to generate rolling forecast to evalaute model performance on the test set. Here, we are using it to generate a forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=endpoint.name,\n",
    "    deployment_name=deployment.name,\n",
    "    inputs={\n",
    "        \"raw_data\": Input(type=AssetTypes.URI_FOLDER, path=\"./data/inference_small\"),\n",
    "        \"train_experiment_name\": Input(type=\"string\", default=experiment_name),\n",
    "        \"instance_count\": Input(type=\"integer\", default=1),\n",
    "        \"max_concurrency_per_instance\": Input(type=\"integer\", default=2),\n",
    "        \"compute_name\": Input(type=\"string\", default=amlcompute_cluster_name),\n",
    "        \"forecast_mode\": Input(type=\"string\", default=\"recursive\"),\n",
    "        \"prs_step_timeout\": Input(type=\"integer\", default=3700),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will stream the job output to monitor the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = job.name\n",
    "batch_job = ml_client.jobs.get(name=job_name)\n",
    "print(f\"Batch job status: {batch_job.status}\\n---\")\n",
    "ml_client.jobs.stream(name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4. Download Forecast Output\n",
    "\n",
    "Next, we download the forecast output and print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(job_name, download_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_df = pd.read_csv(output_file, parse_dates=[time_column_name])\n",
    "fcst_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
