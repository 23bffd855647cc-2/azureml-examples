{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network with PyTorch on the Iris dataset\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace. [Check this notebook for creating a workspace](/sdk/resources/workspace/workspace.ipynb) \n",
    "- A Compute Cluster. [Check this notebook to create a compute cluster](/sdk/resources/compute/compute.ipynb)\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Create and run a `CommandJob` which executes a Python command\n",
    "- Use a local file as an `input` to the CommandJob\n",
    "\n",
    "**Motivations** - This notebook explains how to setup and run a CommandJob. The CommandJob is a fundamental construct of Azure Machine Learning. It can be used to run a task on a specified compute (either local or on the cloud). The CommandJob accepts `environment` and `compute` to setup required infrastructure. You can define a `command` to run on this infrastructure with `inputs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from azure.ml import MLClient\n",
    "from azure.ml.entities import CommandJob, JobInput\n",
    "from azure.identity import InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need a subscription, resource group and workspace name. We will use these details to get a handle to the required Azure Machine Learning workspace use ths `MLClient` from `azure.ml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter details of your AML workspace\n",
    "subscription_id = '<SUBSCRIPTION_ID>'\n",
    "resource_group = '<RESOURCE_GROUP>'\n",
    "workspace = '<AML_WORKSPACE_NAME>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a handle to the workspace\n",
    "ml_client = MLClient(InteractiveBrowserCredential(), subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure and run the CommandJob\n",
    "In this section we will configure and run the CommandJob\n",
    "\n",
    "## 2.1 Configure the CommandJob\n",
    "The command job needs the following to be setup\n",
    "- `code_local_path` - This is the path where the code to run the command is located\n",
    "- `command` - This is the command that needs to be run\n",
    "- `inputs` - This is the dictionary of inputs to the CommandJob. To send it files or folders, we can use the `JobInput` class which  can be used to configure inputs of 3 types: `file`, `folder` or `dataset`.\n",
    "    - `file` - This can be used for local files or remote files. For remote files - http/https, wasb are supported. The example below uses a remote file as input\n",
    "    - `folder` - This can be used for local folders or remote folders. For remote files - http/https, wasb are supported\n",
    "    - `dataset` - To use datasets as input, you can use a registered dataset in the workspace using the format '<dataset_name>:<version>' OR you can use a local file or folder as a dataset. For e.g JobInput(dataset='my_dataset:1') OR JobInput(dataset=Dataset(local_path=\"./data\"))\n",
    "- `environment` - This is the environment needed for the command to run. Curated or custom environments from the workspace can be used. Or a custom environment can be created and used as well. Check out the [environment](/sdk/assets/environment/environment.ipynb) notebook for more examples.\n",
    "- `compute` - The compute on which the CommandJob will run. In this example we are using a compute called `cpu-cluster` present in the workspace. You can replace it any other compute in the workspace. You can run it on the local machine by using `local` for the compute. This will run the CommandJob on the local machine and all the run details and output of the job will be uploaded to the Azure ML workspace.\n",
    "- `display_name` - The display name of the Job\n",
    "- `description` - The description of the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = CommandJob(\n",
    "    code_local_path=\"./src\", #local path where the code is stored\n",
    "    command = \"pip install -r requirements.txt && python main.py --iris-csv ${{inputs.iris_csv}} --epochs ${{inputs.epochs}} --lr ${{inputs.lr}}\",\n",
    "    inputs = {\n",
    "        'iris_csv': JobInput(file='https://azuremlexamples.blob.core.windows.net/datasets/iris.csv'),\n",
    "        'epochs': 10, 'lr': 0.1},\n",
    "    environment = 'AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9',\n",
    "    compute = 'cpu-cluster',\n",
    "    display_name = 'pytorch-iris-example',\n",
    "    description = 'Train a neural network with PyTorch on the Iris dataset.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run the CommandJob\n",
    "Using the `MLClient` created earlier, we will now run this CommandJob in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the command job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "#get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of running a job [here](/sdk/jobs/single-step/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45ee23ad53d8447c1a4a7f9f605254595f8ee53c2e1723e7948bbd485e96ca91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
