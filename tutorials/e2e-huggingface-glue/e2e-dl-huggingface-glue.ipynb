{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbce700",
   "metadata": {},
   "source": [
    "# Using Azure ML Pipelines to Finetune HuggingFace models for GLUE Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe4d4d",
   "metadata": {},
   "source": [
    "**Learning Objectives** \n",
    "By the end of this two part tutorial, you should be able to use Azure Machine Learning (AML) to finetune Hugging Face NLP models.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00c3c3",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "In order to benefit from this tutorial, you need to have:\n",
    "- basic understanding of Machine Learning projects workflow\n",
    "- an Azure subscription. If you don't have an Azure subscription, [create a free account](https://aka.ms/AMLFree) before you begin.\n",
    "- a working AML workspace. A workspace can be created via Azure Portal, Azure CLI, or Python SDK. [Read more](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\n",
    "- a Python environmnet\n",
    "- installed Azure Machine Learning Python SDK v2\n",
    "```python\n",
    "pip install azure-ml==0.0.139 --extra-index-url  https://azuremlsdktestpypi.azureedge.net/sdk-cli-v2\n",
    "```\n",
    "- familiarity with Hugging Face framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113b218",
   "metadata": {},
   "source": [
    "**Motivations** \n",
    "In this tutorial, we will create an AML pipeline to finetune a huggingface model in AML. Specifically, we finetune a light bert model to perform [GLUE tasks](https://gluebenchmark.com/)). Here we have picked [Microsoft Research Paraphrase Corpus](https://gluebenchmark.com/tasks) (mrpc) task for demostration, the code can wasily be changed to work for other purposes.\n",
    "\n",
    "The finetune task is performed in a single python file, which we run inside an AML Command Job. We then use AML's built-in Hyperparameter optimization to get the best performance from our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fe006",
   "metadata": {},
   "source": [
    "### Connect to AzureML\n",
    "\n",
    "Before we dive in the code, we'll need to create an instance of MLClient to connect to Azure ML. Please provide the references to your workspace below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e510cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle to the workspace\n",
    "from azure.ml import MLClient\n",
    "\n",
    "# authentication package\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    InteractiveBrowserCredential(),\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398f7b3",
   "metadata": {},
   "source": [
    "### Provision the required resources for this notebook\n",
    "We'll need a compute clusters for this notebook, you can use a CPU cluster or a GPU cluster. First, let's create a minimal clusters for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9856f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your prefered compute type here\n",
    "USE_GPU = True\n",
    "\n",
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "# Let's create the AML compute object with the intended parameters\n",
    "cluster_basic = AmlCompute(\n",
    "    # Name assigned to the compute cluster\n",
    "    name= \"gpu-cluster\" if USE_GPU else \"cpu-cluster\",\n",
    "    \n",
    "    # AML Compte is AML's on-demand VM service\n",
    "    type=\"amlcompute\",\n",
    "   \n",
    "    # VM Family: 1 x NVIDIA Tesla K80 or 14 GB RAM, 4 CPU VM\n",
    "    size= \"Standard_NC6\" if USE_GPU else \"Standard_DS3_v2\",\n",
    "    \n",
    "    # Minimum running nodes when there is no job running\n",
    "    min_instances=0,\n",
    "    \n",
    "    # nodes in cluster\n",
    "    max_instances=6,\n",
    "    \n",
    "    # How many seconds will the node running after the job termination\n",
    "    idle_time_before_scale_down=120,\n",
    "    \n",
    "    # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination \n",
    "    tier='Dedicated'\n",
    ")\n",
    "\n",
    "# Now, we pass the object to clinet's create_or_update method\n",
    "cluster_basic = ml_client.begin_create_or_update(cluster_basic)\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {cluster_basic.name} is created, the compute size is {cluster_basic.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccf792",
   "metadata": {},
   "source": [
    "# 1. Preparing the Resources\n",
    "\n",
    "## 1.1. Create a Job Environment\n",
    "So far, in the requirements section, we have created a development environment on our development machine. AML needs to know what environment to use for each step of the pipeline. We can use any published docker image as is, or add or required dependencies to the image.In this example, we create a conda environment for our jobs, using a [conda yaml file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually) and add it to an Ubuntu image in Microsoft Container Registry. For more information on AML environments and Azure Container Registries, please check [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02858031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "custom_env_name = \"transformers-gpu\" if USE_GPU else \"transformers-cpu\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for transformer model training\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\", \"azureml-defaults\": \"1.38.0\"},\n",
    "    conda_file=os.path.join(\"components\",\"finetune\", \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04\"\n",
    "    if USE_GPU\n",
    "    else \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220218.v1\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is created, the version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e89b69",
   "metadata": {},
   "source": [
    "## 1.2. Create or Load Components\n",
    "Now that we have our workspace, compute and input data ready, let's work on the individual steps of our pipeline. \n",
    "\n",
    "### 1.2.1 GLUE Component\n",
    "We have created a python script to handle the task of loading the dataset, loading the target model and its weights, trainng and evaluating the model. Here we use the general purpose **CommandComponent** which runs command line action that can be directly calling system commands or running a script. In this example, we use a python script to finetune the Hugging Face model.  The inputs/outputs are accessible in the command via the `${{parameter}}` notation. In this script, we are using `transformers.HfArgumentParser` for argument parsing, which allows up to set all the training parameters in command line. The `${{inputs}}` in the command line can be set via the inputs section of the job. We can use the command line parsing, to specify any extra arguments in the `additional_args` using the `--arg value` format. \n",
    "\n",
    "Once the model is trained, it is registered into AML, so that it can be used in future inference tasks.\n",
    "\n",
    "Please refer to the commented `finerune_glue.py` for the finetuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the CommandComponent Package\n",
    "from azure.ml.entities import CommandComponent, Code\n",
    "\n",
    "src_dir = \"components/finetune/\"\n",
    "\n",
    "\n",
    "glue_component = CommandComponent(\n",
    "    # Name of the component\n",
    "    name=\"GLUE_mrpc\",\n",
    "    \n",
    "    # Component Version, no Version and the component will be automatically versioned \n",
    "    # version=\"26\",\n",
    "    \n",
    "    # The dictionary of the inputs. Each item is a dictionary itself.\n",
    "    inputs=dict(\n",
    "        model_checkpoint=dict(type=\"string\"),\n",
    "        learning_rate=dict(type=\"number\", default=2e-5),\n",
    "        num_train_epochs=dict(type=\"integer\", default=5),\n",
    "        per_device_train_batch_size=dict(type=\"integer\", default=16),\n",
    "        per_device_eval_batch_size=dict(type=\"integer\", default=16),\n",
    "        additional_args=dict(type=\"string\", default=\"\")\n",
    "    ),\n",
    "    \n",
    "    # The dictionary of the outputs. Each item is a dictionary itself.\n",
    "    outputs=dict(trained_model=dict(type=\"path\"),\n",
    "    ),\n",
    "    \n",
    "    # The source folder of the component\n",
    "    code=Code(local_path=src_dir),\n",
    "    \n",
    "    # The environment the component job will be using\n",
    "#     environment=pipeline_job_env,\n",
    "    environment=\"transformers-gpu:1\" if USE_GPU else \"transformers-cpu:1\",\n",
    "    \n",
    "    # The command that will be run in the component\n",
    "    command=\"python finetune_glue.py --model_checkpoint ${{inputs.model_checkpoint}} --output_dir outputs \"\n",
    "    \"--model_checkpoint ${{inputs.model_checkpoint}} --num_train_epochs ${{inputs.num_train_epochs}} \"\n",
    "    \"--learning_rate ${{inputs.learning_rate}} --per_device_train_batch_size ${{inputs.per_device_train_batch_size}} --per_device_eval_batch_size ${{inputs.per_device_eval_batch_size}} \"\n",
    "    \"--disable_tqdm True --trained_model %{{outputs.trained_model}} ${{inputs.additional_args}}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586ede9",
   "metadata": {},
   "source": [
    "# 2. Finetune Hugging Face Model in AML\n",
    "\n",
    "## 2.1 Creating Azure ML Pipeline\n",
    "The created component can be used in a pipeline to be connected to other steps if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "from azure.ml.entities import Component\n",
    "from pathlib import Path\n",
    "\n",
    "glue_func = dsl.load_component(component = glue_component)\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"glue-example_pipeline\",\n",
    "    compute=\"gpu-cluster\" if USE_GPU else \"cpu-cluster\",\n",
    "    description=\"mrpc GLUE Finetune pipeline\",\n",
    ")\n",
    "def glue_pipeline(\n",
    "    pipeline_job_model_checkpoint,\n",
    "    pipeline_job_learning_rate,\n",
    "    pipeline_job_num_train_epochs,\n",
    "    pipeline_job_per_device_train_batch_size,\n",
    "    pipeline_job_per_device_eval_batch_size,\n",
    "    pipeline_job_additional_args,\n",
    "):\n",
    "    glue_step = glue_func(\n",
    "        model_checkpoint=pipeline_job_model_checkpoint,\n",
    "        learning_rate=pipeline_job_learning_rate,\n",
    "        num_train_epochs=pipeline_job_num_train_epochs,\n",
    "        per_device_train_batch_size=pipeline_job_per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=pipeline_job_per_device_eval_batch_size,\n",
    "        additional_args=pipeline_job_additional_args,\n",
    "    )\n",
    "    \n",
    "    return {\"model\": glue_step.outputs.trained_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5e0fc",
   "metadata": {},
   "source": [
    "Let's now use our pipeline definition to instantiate a pipeline with the parameters we choose for our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e766b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = glue_pipeline(\n",
    "    pipeline_job_model_checkpoint=\"distilbert-base-uncased\",\n",
    "    pipeline_job_learning_rate=2e-5,\n",
    "    pipeline_job_num_train_epochs=1,\n",
    "    pipeline_job_per_device_train_batch_size=16,\n",
    "    pipeline_job_per_device_eval_batch_size=16,\n",
    "    pipeline_job_additional_args=\"--seed 37\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192bb5b",
   "metadata": {},
   "source": [
    "## 2.2. Submitting a Finetuning Job to AML Workspace\n",
    "It is now time to submit the job for running in AML. This time we use `create_or_update`  on `ml_client.jobs`. Here we also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in AML studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    \n",
    "    # Project's name\n",
    "    experiment_name=\"glue-example\",\n",
    "    \n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "# get a URL for the status of the job\n",
    "webbrowser.open(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827692b",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Optimization\n",
    "\n",
    "## 3.1. Run a Sweep Job on a Command Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe19d6",
   "metadata": {},
   "source": [
    "Sweep jobs are designed for hyper parameter tuning. Data scientists can define the search space for each hyper parameter (e.g. provide a list of values to pick from or a distribution from which to sample) as well as the objective and the algorithm. The system will then run many jobs (as many as the user specified) with different parameter combinations. The output of a sweep job is the list of outputs of the run that yielded the best results. \n",
    "\n",
    "A component can be used as the trial function of a sweep job. Search Space is a dictionary where the Hyperparameter distribution is defined. Discrete values can be assigned using `Choice`, `QUniform`, `QNormal`, etc. Continuous hyperparameters can be defined by `Normal`, `LogNormal`, `LogUniform` or`Uniform` distributions.\n",
    "\n",
    "The resource budget for trial runs are defined in the `SweepJobLimits`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from azure.ml.entities import CommandJob, JobInput, SweepJob, Choice, Uniform, SweepJobLimits, TruncationSelectionPolicy, Objective, CommandComponent\n",
    "\n",
    "\n",
    "# Each trial job will be provided with a different combination of hyperparameter values that the system samples from the search_space. \n",
    "search_space = {'learning_rate': Uniform(min_value=0.01, max_value=0.9)}\n",
    "\n",
    "#define the limits for this sweep\n",
    "limits = SweepJobLimits(max_total_trials=4, max_concurrent_trials=2, timeout=7200)\n",
    "\n",
    "# set the sampling algorithm for trials, Random sampling, Grid sampling, Bayesian sampling can be chosen\n",
    "sampling_algorithm ='random'\n",
    "\n",
    "# Secify the primary metric you want hyperparameter tuning to optimize. \n",
    "objective=Objective(goal='Minimize', primary_metric='loss')\n",
    "\n",
    "# The early termination policy uses the primary metric to identify low-performance runs.\n",
    "early_termination = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601d99b",
   "metadata": {},
   "source": [
    "Once the search space, budget, sampling algorithm and optimization objective are set, we can create our Sweep job. AML will generate our trials and identify the most performing hyperparameter combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sweep using this component\n",
    "inputs={\n",
    "    \"model_checkpoint\": \"distilbert-base-uncased\",\n",
    "    \"learning_rate\": 2e-5, # We will overwrite it with search space\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"additional_args\":\"--seed 37\"\n",
    "}\n",
    "\n",
    "cmd_sweep_job = SweepJob(\n",
    "    trial=glue_component,\n",
    "    compute=\"gpu-cluster\" if USE_GPU else \"cpu-cluster\",\n",
    "    sampling_algorithm=sampling_algorithm,\n",
    "    inputs=inputs,\n",
    "    search_space=search_space,\n",
    "    objective=objective,\n",
    "    limits=limits,\n",
    "    early_termination=early_termination\n",
    "    display_name='sweep job on glue task',\n",
    "    experiment_name='glue-example',\n",
    "    description='Run a hyperparameter sweep job using component for GLUE mrpc.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the sweep job\n",
    "returned_sweep_job_cmd = ml_client.create_or_update(cmd_sweep_job)\n",
    "#get a URL for the status of the job\n",
    "returned_sweep_job_cmd.services[\"Studio\"].endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk2.0",
   "language": "python",
   "name": "sdk2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
