{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to fine tune a foundation model on Azure Machine Learning using SDK v2\n",
    "\n",
    "Fine tuning a [foundational model](https://learn.microsoft.com/en-us/azure/machine-learning/concept-foundation-models?view=azureml-api-2) has several advantages:\n",
    " \n",
    "* A foundation model may not be optimized for your specific use case, and fine tuning would allow you to customize it for your needs and better performance.\n",
    "* Fine tuning allows you to incorporate your own data into the model, resulting in better accuracy and more relevant results. \n",
    "* Training on your own data could also reduce bias and be more reflective of the unique characteristics of your domain. \n",
    "\n",
    "Ultimately, fine tuning gives you a competitive edge on your product. Customizing the model to your specific needs can make a big difference in your product experience. \n",
    "\n",
    "In this tutorial, you'll walk through the steps to fine tune a natural language processing (NLP) model to analyze sentiments expressed in single sentences written in English.  The tutorial uses the `emotion dataset` and `text-classification` components from the Azure Machine Learning system registry. \n",
    "\n",
    "By the end of this tutorial, you'll have the fine tuned model deployed to an online endpoint for real time inference, which can classify input texts into one of the six emotions: anger, fear, joy, love, sadness, and surprise.  Let's get started!  \n",
    "\n",
    "The steps are:\n",
    "\n",
    ">* Pick a model to fine tune\n",
    ">* Setup pre-requisites such as compute\n",
    ">* Pick and explore training data\n",
    ">* Configure & submit the fine tuning job\n",
    ">* Review training and evaluation metrics\n",
    ">* Register the fine tuned model\n",
    ">* Deploy the fine tuned model for real time inference\n",
    ">* Clean up resources\n",
    "\n",
    "**Training data**\n",
    "\n",
    "You'll use the [emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset. A copy of this dataset is available in the [emotion-dataset](./emotion-dataset/) folder. \n",
    "\n",
    "**Model**\n",
    "\n",
    "Models that can perform the `fill-mask` task are generally good foundation models to fine tune for `text-classification`. We will use the `bert-base-uncased` model in this notebook. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Open in studio and select a compute instance.\n",
    "    * If you opened this notebook from Azure Machine Learning studio, you need a compute instance to run the code. If you don't have a compute instance, select **Create compute** on the toolbar to first create one.  You can use all the default settings.  \n",
    "    \n",
    "    ![Screenshot shows how to create a compute instance.](../get-started-notebooks/media/create-compute.png)\n",
    "    \n",
    "    * If you're seeing this notebook elsewhere, complete [Create resources you need to get started](https://docs.microsoft.com/azure/machine-learning/quickstart-create-resources) to create an Azure Machine Learning workspace and a compute instance.\n",
    "    \n",
    "1. View your VM quota and ensure you have enough quota available to create online deployments. In this tutorial, you will need at least 12 cores of `Standard_NC6s_v3` and 4 cores of `Standard_DS3_v2`. @@IS THIS RIGHT?@@ To view your VM quota usage and request quota increases, see [Manage resource quotas](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas#view-your-usage-and-quotas-in-the-azure-portal).  \n",
    "\n",
    "## Set your kernel\n",
    "\n",
    "* If your compute instance is stopped, start it now.  \n",
    "        \n",
    "    ![Screenshot shows where to start the compute instance.](../get-started-notebooks/media/start-compute.png)\n",
    "\n",
    "* Once your compute instance is running, make sure the that the kernel, found on the top right, is `Python 3.10 - SDK v2`.  If not, use the dropdown to select this kernel.\n",
    "\n",
    "    ![Screenshot shows setting the kernel.](../get-started-notebooks/media/set-kernel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a model to fine tune\n",
    "\n",
    "For `text-classification`, models that support `fill-mask` tasks are good candidates because they're pretrained language models that can understand the context of a given text and predict the missing words or tokens in it. This ability to understand the context of a text and predict missing words make `fill-mask` models highly effective in capturing the meaning of the text and identifying its underlying sentiment or emotion.\n",
    "\n",
    "Let's select a model to fine tune.\n",
    "\n",
    "1. Sign into [Azure Machine Learning studio](ml.azure.com)\n",
    "2. Select `model catalog` on the left navigation bar\n",
    "3. Search for `bert-base-uncased` on the model catalog\n",
    "4. Select the `bert-base-uncased` model to see the model card \n",
    "\n",
    "![Screenshot of the model catalog.](./media/model_catalog.png)\n",
    "\n",
    "On the model card, you can find the model name `bert-base-uncased`. This is the only reference you need in order to fine tune the model on the Notebook using SDK v2. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your workstation for fine tuning\n",
    "\n",
    "Set up your workstation so you can use `Azure Machine Learning SDK v2` to fine tune the model. Follow these steps: \n",
    "\n",
    "### Install dependencies.\n",
    "\n",
    "Install dependencies by running the next cell. This isn't an optional step if running in a new environment. @@DO WE NEED ALL OF THESE ON A COMPUTE INSTANCE?@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-ml in /root/miniconda3/envs/hf/lib/python3.8/site-packages (1.6.0)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.10.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (12.13.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (4.16.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (4.4.0)\n",
      "Requirement already satisfied: pydash<6.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (5.1.2)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (0.7.1)\n",
      "Requirement already satisfied: colorama<0.5.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (0.4.6)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (1.3.2)\n",
      "Requirement already satisfied: azure-storage-file-datalake<13.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (12.9.1)\n",
      "Requirement already satisfied: isodate in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (0.6.1)\n",
      "Requirement already satisfied: strictyaml<2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (1.6.2)\n",
      "Requirement already satisfied: opencensus-ext-azure<2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (1.1.9)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (1.1.28)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (3.19.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (6.0)\n",
      "Requirement already satisfied: azure-storage-file-share<13.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (12.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (4.62.3)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-ai-ml) (1.26.3)\n",
      "Requirement already satisfied: requests>=2.18.4 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-ml) (2.27.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-storage-blob<13.0.0,>=12.10.0->azure-ai-ml) (39.0.0)\n",
      "Collecting azure-storage-blob<13.0.0,>=12.10.0\n",
      "  Using cached azure_storage_blob-12.16.0-py3-none-any.whl (387 kB)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (5.10.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.18.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.5->azure-ai-ml) (23.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (2022.9.24)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (1.3.0)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (5.9.0)\n",
      "Requirement already satisfied: opencensus<1.0.0,>=0.11.2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (0.11.2)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.5.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from strictyaml<2.0.0->azure-ai-ml) (2.8.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.21.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.10.0->azure-ai-ml) (1.15.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (3.9.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.11.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.0->azure-ai-ml) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.0->azure-ai-ml) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.0->azure-ai-ml) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.1.1)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.10.0->azure-ai-ml) (2.21)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (3.19.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.17.3)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.7.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.2->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.4.8)\n",
      "Installing collected packages: azure-storage-blob\n",
      "  Attempting uninstall: azure-storage-blob\n",
      "    Found existing installation: azure-storage-blob 12.13.0\n",
      "    Uninstalling azure-storage-blob-12.13.0:\n",
      "      Successfully uninstalled azure-storage-blob-12.13.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-mlflow 1.49.0 requires azure-storage-blob<=12.13.0,>=12.5.0, but you have azure-storage-blob 12.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed azure-storage-blob-12.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: azure-identity in /root/miniconda3/envs/hf/lib/python3.8/site-packages (1.12.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.11.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity) (1.26.3)\n",
      "Requirement already satisfied: cryptography>=2.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity) (39.0.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity) (1.21.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity) (1.16.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (4.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from cryptography>=2.5->azure-identity) (1.15.1)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msal<2.0.0,>=1.12.0->azure-identity) (2.3.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity) (2.7.0)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2022.9.24)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets==2.9.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (1.22.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (2022.1.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (4.62.3)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (0.70.12.2)\n",
      "Requirement already satisfied: dill<0.3.7 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (0.3.4)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (1.5.3)\n",
      "Requirement already satisfied: responses<0.19 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (11.0.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from datasets==2.9.0) (0.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (4.4.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (3.4.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.9.0) (1.26.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from aiohttp->datasets==2.9.0) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from pandas->datasets==2.9.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from pandas->datasets==2.9.0) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.9.0) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mlflow in /root/miniconda3/envs/hf/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: pytz<2024 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (2021.3)\n",
      "Requirement already satisfied: numpy<2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (1.22.1)\n",
      "Requirement already satisfied: scipy<2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (1.7.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (0.4.2)\n",
      "Requirement already satisfied: matplotlib<4 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (3.6.1)\n",
      "Requirement already satisfied: gunicorn<21 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (20.1.0)\n",
      "Requirement already satisfied: pyarrow<12,>=4.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (11.0.0)\n",
      "Requirement already satisfied: Flask<3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (2.1.1)\n",
      "Requirement already satisfied: docker<7,>=4.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (5.0.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (1.4.35)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (3.1.27)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (0.16.6)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (2.27.1)\n",
      "Requirement already satisfied: querystring-parser<2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (1.0.2)\n",
      "Requirement already satisfied: packaging<24 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (23.1)\n",
      "Requirement already satisfied: pandas<3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (1.5.3)\n",
      "Requirement already satisfied: cloudpickle<3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (1.7.7)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (3.19.6)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (4.11.4)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (3.3.6)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: entrypoints<1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (8.0.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow) (6.0)\n",
      "Requirement already satisfied: Mako in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: importlib-resources in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow) (5.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from docker<7,>=4.0.0->mlflow) (1.3.2)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from Flask<3->mlflow) (2.0.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from Flask<3->mlflow) (2.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.9)\n",
      "Requirement already satisfied: setuptools>=3.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from gunicorn<21->mlflow) (63.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from matplotlib<4->mlflow) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from matplotlib<4->mlflow) (4.37.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from matplotlib<4->mlflow) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (3.0.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (1.1.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: azureml-mlflow in /root/miniconda3/envs/hf/lib/python3.8/site-packages (1.49.0)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (0.7.1)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (1.3.2)\n",
      "Requirement already satisfied: azure-identity in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (1.12.0)\n",
      "Collecting azure-storage-blob<=12.13.0,>=12.5.0\n",
      "  Using cached azure_storage_blob-12.13.0-py3-none-any.whl (377 kB)\n",
      "Requirement already satisfied: azure-core!=1.22.0,<2.0.0,>=1.8.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (1.26.3)\n",
      "Requirement already satisfied: cryptography in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (39.0.0)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (1.1.28)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (2.8.2)\n",
      "Requirement already satisfied: jsonpickle in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (3.0.1)\n",
      "Requirement already satisfied: mlflow-skinny in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azureml-mlflow) (2.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (4.4.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (2.27.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from cryptography->azureml-mlflow) (1.15.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msrest>=0.6.18->azureml-mlflow) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msrest>=0.6.18->azureml-mlflow) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msrest>=0.6.18->azureml-mlflow) (2022.9.24)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity->azureml-mlflow) (1.0.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from azure-identity->azureml-mlflow) (1.21.0)\n",
      "Requirement already satisfied: pytz<2023 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (2021.3)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (3.19.6)\n",
      "Requirement already satisfied: entrypoints<1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (0.4)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (6.0)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (4.11.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (8.0.3)\n",
      "Requirement already satisfied: cloudpickle<3 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (2.0.0)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (0.16.6)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (0.4.2)\n",
      "Requirement already satisfied: packaging<24 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (23.1)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from mlflow-skinny->azureml-mlflow) (3.1.27)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from cffi>=1.12->cryptography->azureml-mlflow) (2.21)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny->azureml-mlflow) (0.8.9)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny->azureml-mlflow) (3.1.1)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny->azureml-mlflow) (2.3.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from gitpython<4,>=2.1.0->mlflow-skinny->azureml-mlflow) (4.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow-skinny->azureml-mlflow) (3.9.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->azureml-mlflow) (2.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.18.4->azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (3.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/hf/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow-skinny->azureml-mlflow) (5.0.0)\n",
      "Installing collected packages: azure-storage-blob\n",
      "  Attempting uninstall: azure-storage-blob\n",
      "    Found existing installation: azure-storage-blob 12.16.0\n",
      "    Uninstalling azure-storage-blob-12.16.0:\n",
      "      Successfully uninstalled azure-storage-blob-12.16.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azure-storage-file-datalake 12.9.1 requires azure-storage-blob<13.0.0,>=12.14.1, but you have azure-storage-blob 12.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed azure-storage-blob-12.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "%pip install datasets==2.9.0\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create handle to workspace\n",
    "Before we dive in the code, you need a way to reference your workspace. Create `ml_client` for a handle to the workspace. Then use `ml_client` to manage resources and jobs.\n",
    "\n",
    "In the next cell, enter your `Subscription ID`, `Resource Group` name and `Workspace` name. To find these values:\n",
    "\n",
    "- In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
    "- Copy the value for workspace, resource group and subscription ID into the code.\n",
    "- You'll need to copy one value, close the area and paste, then come back for the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    "    ClientSecretCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"ed2cab61-14cc-4fb3-ac23-d72609214cfd\",\n",
    "        resource_group_name=\"training_rg\",\n",
    "        workspace_name=\"swatig_ws\",\n",
    "    )\n",
    "\n",
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "compute_cluster = \"gpu-cluster-big\"\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "except Exception as ex:\n",
    "    compute = AmlCompute(\n",
    "        name=compute_cluster,\n",
    "        size=\"Standard_NC24rs_v3\",\n",
    "        max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "    )\n",
    "    workspace_ml_client.compute.begin_create_or_update(compute).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to `azureml` system registry & import the model\n",
    "\n",
    "In order to access the preregistered foundation models hosted on the model catalog, you need to connect to `azureml` registry. Run the next cell to connect to the system registry and import the `bert-base-uncased` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using model name: bert-base-uncased, version: 3, id: azureml://registries/azureml/models/bert-base-uncased/versions/3 for fine tuning\n"
     ]
    }
   ],
   "source": [
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model_version = \"3\"\n",
    "foundation_model = registry_ml_client.models.get(model_name, model_version)\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set an optional experiment name\n",
    "This step is optional but useful if you want to find this fine tuning job easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"text-classification-emotion-detection\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check or create a compute cluster  @@OR USE SERVERLESS???@@\n",
    "\n",
    "For fine tuning tasks, you need a GPU compute cluster for the best results. The duration of the fine tuning depends on the capacity of the GPU SKU you choose. That is because a single GPU node can have multiple GPU cards. \n",
    "\n",
    "For example, in one node of `Standard_ND40rs_v2` there are eight NVIDIA GPUs. Meanwhile in `Standard_NC12s_v2` there are two NVIDIA V100 GPUs. When all GPUs in the node get utilized (by configuring the parameter in `gpus_per_node`), you get the most efficient fine tune run. You can read more about Azure's [GPU optimized VM offerings](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) and the recommended compute SKUs ([ncv3-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series), [ndv2-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series)).\n",
    "\n",
    "In this tutorial, you'll use `Standard_NC6s_v3` which takes about 15-20 minutes to complete the fine tuning run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU's in copute STANDARD_ND40RS_V2: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute.\n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpu_count_found = False\n",
    "workspace_compute_sku_list = workspace_ml_client.compute.list_sizes()\n",
    "available_sku_sizes = []\n",
    "for compute_sku in workspace_compute_sku_list:\n",
    "    available_sku_sizes.append(compute_sku.name)\n",
    "    if compute_sku.name.lower() == compute.size.lower():\n",
    "        gpus_per_node = compute_sku.gpus\n",
    "        gpu_count_found = True\n",
    "# if gpu_count_found not found, then print an error\n",
    "if gpu_count_found:\n",
    "    print(f\"Number of GPU's in copute {compute.size}: {gpus_per_node}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Number of GPU's in copute {compute.size} not found. Available skus are: {available_sku_sizes}.\"\n",
    "        f\"This should not happen. Please check the selected compute cluster: {compute_cluster} and try again.\"\n",
    "    )\n",
    "# CPU based finetune works only for single-node single-process\n",
    "if gpus_per_node == 0:\n",
    "    print(\n",
    "        \"WARNING! Selected compute doesn't have GPU. CPU based finetune is experimental and works on a single process in a single node\"\n",
    "    )\n",
    "    gpus_per_node = 1\n",
    "\n",
    "# genrating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset for fine-tuning the model\n",
    "\n",
    "There are two options to prepare the dataset for fine tuning. The first option is to choose the fine tune option on the model catalog where you found ` bert-base-uncased` model earlier. The second option is to prepare a dataset that matches your use case for fine tuning. This tutorial focuses on the second option.  \n",
    "\n",
    "You're going to use the [emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset. You can find a copy of this dataset in the emotion-dataset folder that came with this notebook. \n",
    "\n",
    "### Start by downloading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 3.97k/3.97k [00:00<00:00, 3.32MB/s]\n",
      "Downloading metadata: 100%|██████████| 3.28k/3.28k [00:00<00:00, 2.69MB/s]\n",
      "Downloading readme: 100%|██████████| 8.78k/8.78k [00:00<00:00, 5.88MB/s]\n",
      "No config specified, defaulting to: emotion/split\n",
      "No config specified, defaulting to: emotion/split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/split to /root/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data: 100%|██████████| 592k/592k [00:00<00:00, 9.54MB/s]\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s]\n",
      "Downloading data: 100%|██████████| 74.0k/74.0k [00:00<00:00, 5.94MB/s]\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:00<00:00,  2.09it/s]\n",
      "Downloading data: 100%|██████████| 74.9k/74.9k [00:00<00:00, 6.37MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 80.49it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 16/16 [00:00<00:00, 244.38ba/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to /root/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/root/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 279.34ba/s]\n",
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/root/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 358.86ba/s]\n",
      "No config specified, defaulting to: emotion/split\n",
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/root/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    }
   ],
   "source": [
    "# download the dataset using the helper script. This needs datasets library: https://pypi.org/project/datasets/\n",
    "import os\n",
    "\n",
    "exit_status = os.system(\"python ./download-dataset.py --download_dir emotion-dataset\")\n",
    "if exit_status != 0:\n",
    "    raise Exception(\"Error downloading dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some data rows\n",
    "It's important to understand the data and its features. Let's start by taking a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplace i will know that it is still on the property</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           text  \\\n",
       "0  i didnt feel humiliated                                                                                        \n",
       "1  i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n",
       "2  im grabbing a minute to post i feel greedy wrong                                                               \n",
       "3  i am ever feeling nostalgic about the fireplace i will know that it is still on the property                   \n",
       "4  i am feeling grouchy                                                                                           \n",
       "\n",
       "   label  \n",
       "0  0      \n",
       "1  0      \n",
       "2  3      \n",
       "3  2      \n",
       "4  3      "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the ./emotion-dataset/train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(\"./emotion-dataset/train.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace numerical categories in data with the actual string labels\n",
    "\n",
    "This data set uses numerical categories. For example, 0 refers to `sadness`. To get string labels such as `anger`, `joy`, etc., replace the categories. Run the next cell to get the string labels.\n",
    "\n",
    "You can see the detailed mapping in the [./emotion-dataset/label.json](./emotion-dataset/label.json). If you skip this step, the model returns numerical categories such as 0, 1, 2, etc. and you have to map them to what the category represents yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label label_string\n",
       "0  0      sadness    \n",
       "1  1      joy        \n",
       "2  2      love       \n",
       "3  3      anger      \n",
       "4  4      fear       "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the id2label json element of the ./emotion-dataset/label.json file into pandas table with keys as 'label' column of int64 type and values as 'label_string' column as string type\n",
    "import json\n",
    "\n",
    "with open(\"./emotion-dataset/label.json\") as f:\n",
    "    id2label = json.load(f)\n",
    "    id2label = id2label[\"id2label\"]\n",
    "    label_df = pd.DataFrame.from_dict(\n",
    "        id2label, orient=\"index\", columns=[\"label_string\"]\n",
    "    )\n",
    "    label_df[\"label\"] = label_df.index.astype(\"int64\")\n",
    "    label_df = label_df[[\"label\", \"label_string\"]]\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplace i will know that it is still on the property</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           text  \\\n",
       "0  i didnt feel humiliated                                                                                        \n",
       "1  i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n",
       "2  im grabbing a minute to post i feel greedy wrong                                                               \n",
       "3  i am ever feeling nostalgic about the fireplace i will know that it is still on the property                   \n",
       "4  i am feeling grouchy                                                                                           \n",
       "\n",
       "   label label_string  \n",
       "0  0      sadness      \n",
       "1  0      sadness      \n",
       "2  3      anger        \n",
       "3  2      love         \n",
       "4  3      anger        "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test.jsonl, train.jsonl and validation.jsonl form the ./emotion-dataset folder into pandas dataframes\n",
    "test_df = pd.read_json(\"./emotion-dataset/test.jsonl\", lines=True)\n",
    "train_df = pd.read_json(\"./emotion-dataset/train.jsonl\", lines=True)\n",
    "validation_df = pd.read_json(\"./emotion-dataset/validation.jsonl\", lines=True)\n",
    "# join the train, validation and test dataframes with the id2label dataframe to get the label_string column\n",
    "train_df = train_df.merge(label_df, on=\"label\", how=\"left\")\n",
    "validation_df = validation_df.merge(label_df, on=\"label\", how=\"left\")\n",
    "test_df = test_df.merge(label_df, on=\"label\", how=\"left\")\n",
    "# show the first 5 rows of the train dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data\n",
    "Now the string labels are applied, let's save the dataset.\n",
    "\n",
    "For the fine tuning tutorial demonstration purposes, you're going to save a smaller dataset containing 10% of the original dataset into `train`, `validation` and `test` files. **Keep in mind that the fine tuned model will have lower accuracy, hence it should not be put to real-world use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10% of the rows from the train, validation and test dataframes into files with small_ prefix in the ./emotion-dataset folder\n",
    "frac = 1\n",
    "train_df.sample(frac=frac).to_json(\n",
    "    \"./emotion-dataset/small_train.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "validation_df.sample(frac=frac).to_json(\n",
    "    \"./emotion-dataset/small_validation.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "test_df.sample(frac=frac).to_json(\n",
    "    \"./emotion-dataset/small_test.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and submit the fine tuning job using the model and data as inputs\n",
    "\n",
    "To submit a fine tuning job using a foundation model, you're going to build a pipeline. There are two reasons for using a pipeline. \n",
    "\n",
    "First, since you're fine tuning an existing foundation model, you may not have access to the training code. Azure Machine Learning can generate the training code, which is hosted in the `azureml` registry, which requires using a pipeline. Second, fine tuning job requires several steps, including tokenization, converting English text to numeric representation, passing tokenized data to fine tune, and evaluation. It would make sense to componentize these discrete steps, building a pipeline.\n",
    "\n",
    "You're going to create a job that uses the `text-classification` pipeline component. \n",
    "\n",
    "This tutorial is fine tuning a model from the `azureml` system registery.  If you instead want to fine tune a model that is available on HuggingFace, but not available in `azureml` system registry, you can either [import](https://github.com/Azure/azureml-examples) the model or use the `huggingface_id` parameter to instruct the components to pull the model directly from [HuggingFace](https://huggingface.co). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"text_classification_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "    text_classification_pipeline = pipeline_component_func(\n",
    "        # specify the foundation model available in the azureml system registry id identified in step #3\n",
    "        mlflow_model_path=foundation_model.id,\n",
    "        # huggingface_id = 'bert-base-uncased', # if you want to use a huggingface model, uncomment this line and comment the above line\n",
    "        compute_model_import=compute_cluster,\n",
    "        compute_preprocess=compute_cluster,\n",
    "        compute_finetune=compute_cluster,\n",
    "        compute_model_evaluation=compute_cluster,\n",
    "        # map the dataset splits to parameters\n",
    "        train_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./emotion-dataset/small_train.jsonl\"\n",
    "        ),\n",
    "        validation_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./emotion-dataset/small_validation.jsonl\"\n",
    "        ),\n",
    "        test_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./emotion-dataset/small_test.jsonl\"\n",
    "        ),\n",
    "        evaluation_config=Input(\n",
    "            type=\"uri_file\", path=\"./text-classification-config.json\"\n",
    "        ),\n",
    "        # The following parameters map to the dataset fields\n",
    "        sentence1_key=\"text\",\n",
    "        label_key=\"label_string\",\n",
    "        # Training settings\n",
    "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        learning_rate=2e-5,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": text_classification_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pipeline job is configured, submit the job.  @@HOW LONG DOES THIS TAKE?@@ DO WE NEED TO WAIT FOR IT TO FINISH?@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading small_train.jsonl\u001b[32m (< 1 MB): 100%|██████████| 2.27M/2.27M [00:02<00:00, 983kB/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading small_validation.jsonl\u001b[32m (< 1 MB): 100%|██████████| 280k/280k [00:00<00:00, 636kB/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading small_test.jsonl\u001b[32m (< 1 MB): 100%|██████████| 283k/283k [00:00<00:00, 707kB/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading text-classification-config.json\u001b[32m (< 1 MB): 100%|██████████| 768/768 [00:00<00:00, 7.97kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: clever_roti_y385lbt48h\n",
      "Web View: https://ml.azure.com/runs/clever_roti_y385lbt48h?wsid=/subscriptions/ed2cab61-14cc-4fb3-ac23-d72609214cfd/resourcegroups/training_rg/workspaces/swatig_ws\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-06-08 20:42:11Z] Submitting 1 runs, first five are: 3fd3b58b:7520ea3e-4b28-408a-b0a1-68ba7b4bf586\n",
      "[2023-06-08 21:08:17Z] Completing processing run id 7520ea3e-4b28-408a-b0a1-68ba7b4bf586.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: clever_roti_y385lbt48h\n",
      "Web View: https://ml.azure.com/runs/clever_roti_y385lbt48h?wsid=/subscriptions/ed2cab61-14cc-4fb3-ac23-d72609214cfd/resourcegroups/training_rg/workspaces/swatig_ws\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review training and evaluation metrics\n",
    "\n",
    "Now the pipeline job is submitted, you can view the job in Azure Machine Learning studio to analyze logs, metrics, and outputs of jobs. This way, you can create custom charts and compare metrics across different fine tuning jobs. See [View jobs/runs information in the studio](https://learn.microsoft.com/azure/machine-learning/how-to-log-view-metrics?tabs=interactive#view-jobsruns-information-in-the-studio) to learn more about job metrics.\n",
    "\n",
    "You may also want to programmatically log the same information so that it can be used by other services. In that case, use the following MLflow code, which is the recommended client for logging and querying metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, json\n",
    "\n",
    "mlflow_tracking_uri = workspace_ml_client.workspaces.get(\n",
    "    workspace_ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "# concat 'tags.mlflow.rootRunId=' and pipeline_job.name in single quotes as filter variable\n",
    "filter = \"tags.mlflow.rootRunId='\" + pipeline_job.name + \"'\"\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name], filter_string=filter, output_format=\"list\"\n",
    ")\n",
    "training_run = None\n",
    "evaluation_run = None\n",
    "# get the training and evaluation runs.\n",
    "# using a hacky way till 'Bug 2320997: not able to show eval metrics in FT notebooks - mlflow client now showing display names' is fixed\n",
    "for run in runs:\n",
    "    # check if run.data.metrics.epoch exists\n",
    "    if \"epoch\" in run.data.metrics:\n",
    "        training_run = run\n",
    "    # else, check if run.data.metrics.accuracy exists\n",
    "    elif \"accuracy\" in run.data.metrics:\n",
    "        evaluation_run = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics:\n",
      "\n",
      "\n",
      "{\n",
      "  \"loss\": 0.1013,\n",
      "  \"learning_rate\": 0.0,\n",
      "  \"epoch\": 3.0,\n",
      "  \"eval_loss\": 0.1942712813615799,\n",
      "  \"eval_accuracy\": 0.9405,\n",
      "  \"eval_f1_macro\": 0.9139922655171362,\n",
      "  \"eval_mcc\": 0.9217377761763651,\n",
      "  \"eval_precision_macro\": 0.9231576560328755,\n",
      "  \"eval_recall_macro\": 0.906752103717829,\n",
      "  \"eval_runtime\": 6.4123,\n",
      "  \"eval_samples_per_second\": 311.899,\n",
      "  \"eval_steps_per_second\": 38.987,\n",
      "  \"train_runtime\": 456.156,\n",
      "  \"train_samples_per_second\": 105.227,\n",
      "  \"train_steps_per_second\": 13.153,\n",
      "  \"total_flos\": 1.2629784051843072e+16,\n",
      "  \"train_loss\": 0.2425000457763672\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if training_run:\n",
    "    print(\"Training metrics:\\n\\n\")\n",
    "    print(json.dumps(training_run.data.metrics, indent=2))\n",
    "else:\n",
    "    print(\"No Training job found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics:\n",
      "\n",
      "\n",
      "{\n",
      "  \"average_precision_score_macro\": 0.9616133732880718,\n",
      "  \"AUC_macro\": 0.993413475301303,\n",
      "  \"recall_score_macro\": 0.8795230156675254,\n",
      "  \"average_precision_score_binary\": NaN,\n",
      "  \"average_precision_score_micro\": 0.9868280556869524,\n",
      "  \"AUC_binary\": NaN,\n",
      "  \"recall_score_micro\": 0.934,\n",
      "  \"AUC_micro\": 0.9961019499999999,\n",
      "  \"norm_macro_recall\": 0.8554276188010306,\n",
      "  \"average_precision_score_weighted\": 0.983899560696251,\n",
      "  \"weighted_accuracy\": 0.9523587534732753,\n",
      "  \"precision_score_micro\": 0.934,\n",
      "  \"f1_score_binary\": NaN,\n",
      "  \"precision_score_macro\": 0.9040907619133997,\n",
      "  \"f1_score_micro\": 0.934,\n",
      "  \"precision_score_weighted\": 0.9336396752210118,\n",
      "  \"f1_score_weighted\": 0.9332863578432629,\n",
      "  \"recall_score_binary\": NaN,\n",
      "  \"matthews_correlation\": 0.9126364916099332,\n",
      "  \"log_loss\": 0.22058146509919424,\n",
      "  \"accuracy\": 0.934,\n",
      "  \"precision_score_binary\": NaN,\n",
      "  \"balanced_accuracy\": 0.8795230156675254,\n",
      "  \"AUC_weighted\": 0.9959533664256791,\n",
      "  \"f1_score_macro\": 0.8893683288869473,\n",
      "  \"recall_score_weighted\": 0.934\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if evaluation_run:\n",
    "    print(\"Evaluation metrics:\\n\\n\")\n",
    "    print(json.dumps(evaluation_run.data.metrics, indent=2))\n",
    "else:\n",
    "    print(\"No Evaluation job found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the fine tuned model with the workspace\n",
    "\n",
    "Register the model from the output of the fine tuning job. There are several benefits to register a fine tuned model to the Azure Machine Learning platform.\n",
    " \n",
    "- **Versioning & Traceability**: Tracks lineage between the fine tuned model and the fine tuning job. The fine tuning job, further, tracks lineage to the foundation model, data and training code.\n",
    "\n",
    "- **Reusability**: Once a model is registered, it can be reused across different experiments, pipelines, and deployments. This eliminates the need to recreate the model each time and saves time and effort.\n",
    "\n",
    "- **Collaboration**: Registered models can be easily shared with other team members, making it easier to collaborate on machine learning projects. This enables team members to work together on the same model and share their insights and feedback. \n",
    "\n",
    "- **Deployment**: Registered models can be easily deployed to production environments, making it easier to integrate machine learning models into business applications. Azure Machine Learning provides several deployment options, including Azure Kubernetes Service, Azure Container Instances, and Azure Functions. \n",
    "\n",
    "- **Monitoring**: Registered models can be monitored and evaluated over time to ensure that they continue to perform well in production environments. This enables you to detect and address issues early on and maintain the performance of your machine learning models.\n",
    "\n",
    "Use the following code to register the fine tuned model. Once registered, you can find the model under the Models tab of Azure Machine Learning studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline job outputs:  {'trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f9b152547f0>}\n",
      "path to register model:  azureml://jobs/clever_roti_y385lbt48h/outputs/trained_model\n",
      "prepare to register model: \n",
      " description: bert-base-uncased fine tuned model for emotion detection\n",
      "name: bert-base-uncased-emotion-detection\n",
      "path: azureml://jobs/clever_roti_y385lbt48h/outputs/trained_model\n",
      "properties: {}\n",
      "tags: {}\n",
      "type: mlflow_model\n",
      "version: '1686256902'\n",
      "\n",
      "registered model: \n",
      " creation_context:\n",
      "  created_at: '2023-06-08T21:09:14.899595+00:00'\n",
      "  created_by: Manoj Bableshwar\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2023-06-08T21:09:14.899595+00:00'\n",
      "  last_modified_by: Manoj Bableshwar\n",
      "  last_modified_by_type: User\n",
      "description: bert-base-uncased fine tuned model for emotion detection\n",
      "flavors:\n",
      "  hftransformersv2:\n",
      "    code: ''\n",
      "    hf_config_class: BertConfig\n",
      "    hf_pretrained_class: BertForSequenceClassification\n",
      "    hf_tokenizer_class: BertTokenizerFast\n",
      "    huggingface_id: bert-base-uncased\n",
      "    model_data: data\n",
      "    pytorch_version: 1.13.1\n",
      "    task_type: text-classification\n",
      "    tokenizer_config: \"{\\n  \\\"max_length\\\": \\\"512\\\",\\n  \\\"padding\\\": \\\"max_length\\\"\\\n",
      "      ,\\n  \\\"truncation\\\": \\\"true\\\"\\n}\"\n",
      "    train_label_list: \"{\\n  \\\"path_list\\\": \\\"train_label_list.npy\\\"\\n}\"\n",
      "    transformers_version: 4.29.1\n",
      "  python_function:\n",
      "    data: data\n",
      "    env: conda.yaml\n",
      "    loader_module: azureml.evaluate.mlflow.hftransformers\n",
      "    python_version: 3.8.16\n",
      "id: azureml:/subscriptions/ed2cab61-14cc-4fb3-ac23-d72609214cfd/resourceGroups/training_rg/providers/Microsoft.MachineLearningServices/workspaces/swatig_ws/models/bert-base-uncased-emotion-detection/versions/1686256902\n",
      "job_name: clever_roti_y385lbt48h\n",
      "name: bert-base-uncased-emotion-detection\n",
      "path: azureml://subscriptions/ed2cab61-14cc-4fb3-ac23-d72609214cfd/resourceGroups/training_rg/workspaces/swatig_ws/datastores/workspaceblobstore/paths/azureml/d5b5294b-024d-435d-831e-b48e41ee0741/mlflow_model_folder/\n",
      "properties: {}\n",
      "tags: {}\n",
      "type: mlflow_model\n",
      "version: '1686256902'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# check if the `trained_model` output is available\n",
    "print(\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job.name).outputs)\n",
    "\n",
    "# fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
    "    pipeline_job.name, \"trained_model\"\n",
    ")\n",
    "\n",
    "finetuned_model_name = model_name + \"-emotion-detection\"\n",
    "finetuned_model_name = finetuned_model_name.replace(\"/\", \"-\")\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=finetuned_model_name,\n",
    "    version=timestamp,  # use timestamp as version to avoid version conflict\n",
    "    description=model_name + \" fine tuned model for emotion detection\",\n",
    ")\n",
    "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
    "# register the model from pipeline job output\n",
    "registered_model = workspace_ml_client.models.create_or_update(\n",
    "    prepare_to_register_model\n",
    ")\n",
    "print(\"registered model: \\n\", registered_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the fine tuned model to an online endpoint\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model. In this tutorial, you're going to use Managed Online Endpoint API, which handles many backend configurations for you.\n",
    "\n",
    "Let's start by creating an online endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "\n",
    "online_endpoint_name = \"emotion-\" + timestamp\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + registered_model.name\n",
    "    + \", fine tuned model for emotion detection\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying a model requires a compute resource. In this tutorial, you're going to use `Standard_DS3_v2` which takes about @@HOW LONG@@ minutes to complete the deployment. \n",
    "\n",
    "You can also read about [the list of other SKUs supported for deployment](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instance type Standard_DS2_v2 may be too small for compute resources. Minimum recommended compute SKU is Standard_DS3_v2 for general purpose endpoints. Learn more about SKUs here: https://learn.microsoft.com/en-us/azure/machine-learning/referencemanaged-online-endpoints-vm-sku-list\n",
      "Check: endpoint emotion-1686256902 exists\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................."
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "(None) ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nCode: None\nMessage: ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nException Details:\t(None) ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"swatigws8304006853\" under the path \"/azureml/ImageLogs/84cb0c35-04d6-4959-8bd1-233100f976c6/build.log\"\n\tCode: None\n\tMessage: ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"swatigws8304006853\" under the path \"/azureml/ImageLogs/84cb0c35-04d6-4959-8bd1-233100f976c6/build.log\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationFailed\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/core/polling/base_polling.py:500\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll()\n\u001b[1;32m    502\u001b[0m \u001b[39mexcept\u001b[39;00m BadStatus \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/core/polling/base_polling.py:538\u001b[0m, in \u001b[0;36mLROBasePolling._poll\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m _failed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus()):\n\u001b[0;32m--> 538\u001b[0m     \u001b[39mraise\u001b[39;00m OperationFailed(\u001b[39m\"\u001b[39m\u001b[39mOperation failed or canceled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    540\u001b[0m final_get_url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_operation\u001b[39m.\u001b[39mget_final_get_url(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline_response)\n",
      "\u001b[0;31mOperationFailed\u001b[0m: Operation failed or canceled",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create a deployment\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m demo_deployment \u001b[39m=\u001b[39m ManagedOnlineDeployment(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdemo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     endpoint_name\u001b[39m=\u001b[39monline_endpoint_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     instance_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m workspace_ml_client\u001b[39m.\u001b[39;49monline_deployments\u001b[39m.\u001b[39;49mbegin_create_or_update(demo_deployment)\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m endpoint\u001b[39m.\u001b[39mtraffic \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdemo\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m100\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bmanojubuntu/mnt/c/CODE/REPOS/azureml-examples/tutorials/fine-tune-foundation-model/fine-tune-foundation-model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m workspace_ml_client\u001b[39m.\u001b[39mbegin_create_or_update(endpoint)\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/core/polling/_poller.py:272\u001b[0m, in \u001b[0;36mLROPoller.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_thread\u001b[39m.\u001b[39mjoin(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39m# Let's handle possible None in forgiveness here\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39m# https://github.com/python/mypy/issues/8165\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# Was None\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/core/polling/_poller.py:189\u001b[0m, in \u001b[0;36mLROPoller._start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m\"\"\"Start the long running operation.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39mOn completion, runs any callbacks.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[39m:param callable update_cmd: The API request to check the status of\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m the operation.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_polling_method\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    190\u001b[0m \u001b[39mexcept\u001b[39;00m AzureError \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    191\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m error\u001b[39m.\u001b[39mcontinuation_token:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.8/site-packages/azure/core/polling/base_polling.py:517\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m HttpResponseError(\n\u001b[1;32m    511\u001b[0m         response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline_response\u001b[39m.\u001b[39mhttp_response,\n\u001b[1;32m    512\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(err),\n\u001b[1;32m    513\u001b[0m         error\u001b[39m=\u001b[39merr,\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    516\u001b[0m \u001b[39mexcept\u001b[39;00m OperationFailed \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mraise\u001b[39;00m HttpResponseError(\n\u001b[1;32m    518\u001b[0m         response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline_response\u001b[39m.\u001b[39mhttp_response, error\u001b[39m=\u001b[39merr\n\u001b[1;32m    519\u001b[0m     )\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (None) ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nCode: None\nMessage: ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nException Details:\t(None) ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"swatigws8304006853\" under the path \"/azureml/ImageLogs/84cb0c35-04d6-4959-8bd1-233100f976c6/build.log\"\n\tCode: None\n\tMessage: ResourceNotReady: User container has crashed or terminated: Liveness probe failed: HTTP probe failed with statuscode: 502. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"swatigws8304006853\" under the path \"/azureml/ImageLogs/84cb0c35-04d6-4959-8bd1-233100f976c6/build.log\""
     ]
    }
   ],
   "source": [
    "# create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint with sample data\n",
    "\n",
    "Now the fine tuned model is deployed, we need to test if the model is working properly. You'll first fetch some sample data from the test dataset, and save as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ./emotion-dataset/small_test.jsonl into a pandas dataframe\n",
    "test_df = pd.read_json(\"./emotion-dataset/small_test.jsonl\", lines=True)\n",
    "# take 10 random samples\n",
    "test_df = test_df.sample(n=10)\n",
    "# rebuild index\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "# rename the label_string column to ground_truth_label\n",
    "test_df = test_df.rename(columns={\"label_string\": \"ground_truth_label\"})\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a json object with the key as \"inputs\" and value as a list of values from the text column of the test dataframe\n",
    "test_df_copy = test_df[[\"text\"]]\n",
    "test_json = {\"input_data\": test_df_copy.to_dict(\"split\")}\n",
    "# save the json object to a file named sample_score.json in the ./emotion-dataset folder\n",
    "with open(\"./emotion-dataset/sample_score.json\", \"w\") as f:\n",
    "    json.dump(test_json, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a sample data, let's test the online endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"demo\",\n",
    "    request_file=\"./emotion-dataset/sample_score.json\",\n",
    ")\n",
    "print(\"raw response: \\n\", response, \"\\n\")\n",
    "# convert the response to a pandas dataframe and rename the label column as scored_label\n",
    "response_df = pd.read_json(response)\n",
    "response_df = response_df.rename(columns={0: \"scored_label\"})\n",
    "response_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the test dataframe and the response dataframe on the index\n",
    "merged_df = pd.merge(test_df, response_df, left_index=True, right_index=True)\n",
    "merged_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the online endpoint\n",
    "Congratulation! You have completed the foundational model fine tuning tutorial.\n",
    "\n",
    "Don't forget to delete the online endpoint, else you'll leave the billing meter running for the compute used by the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
